

	      mpiP: Lightweight, scalable MPI profiling

			    Jeffrey Vetter
			  (vetter3@llnl.gov)

			   Chris Chambreau
			  (chcham@llnl.gov)

			     Version 2.5

			   14 August, 2003
				   


Contents

    * Introduction 
    * Contributing 
    * New Features 
    * User Guide (summary) 
    * Supported Platforms 
    * Building mpiP 
    * Runtime Configuration 
    * mpiP Output 
    * Controlling mpiP profiling scope 
    * Caveats 
    * List of Profiled Routines 
    * List of Routine with collected sent message size information
      

------------------------------------------------------------------------
Introduction

mpiP is a lightweight profiling library for MPI applications. Because it
only collects statistical information about MPI functions, mpiP
generates considerably less overhead and much less data than tracing
tools. All the information captured by mpiP is task-local. It only uses
communication at the end of the application experiment to merge results
from all of the tasks into one output file.

We have tested mpiP on a variety of C/C++/Fortran applications from 2 to
1536 tasks. Please send your comments, questions, and ideas for
enhancements to vetter3@llnl.gov <mailto:vetter3@llnl.gov>.

To learn more about performance analysis with mpiP, see Vetter, J.S. and
M.O. McCracken, "Statistical Scalability Analysis of Communication
Operations in Distributed Applications
<http://www.llnl.gov/CASC/people/vetter/vetter_pubs.html>," Proc. ACM
SIGPLAN Symp. on Principles and Practice of Parallel Programming
(PPOPP), 2001.

The most up-to-date version of this document can be found at
http://www.llnl.gov/casc/mpip/.


Contributing

We are constantly improving mpiP. Bug fixes and ports to new platforms
are always welcome.

Many thanks to the following contributors:

    * Michael McCracken (UCSD).
    * Curt Janssen (Sandia National Laboratory).
    * Chris Chambreau (Lawrence Livermore National Laboratory).
    * Mike Campbell (UIUC).


New Features with v2.5

Release v2.5 corrects some outstanding issues and provides the following
new features:

    * Message Size Reporting
    * C++ name demangling
    * Long function and file name support


------------------------------------------------------------------------
Using mpiP

Using mpiP is very simple. Because it gathers MPI information through
the MPI profiling layer, mpiP is a link-time library. That is, you don't
have to recompile your application to use mpiP. Note that you might have
to recompile to include the '-g' option. This is important if you want
mpiP to decode the PC to a source code filename and line number
automatically. mpiP will work without '-g', but mileage may vary.

To compile a simple program on LLNL AIX, you need to add the following
libraries to your compile/link line:

   -L${mpiP_root}/lib -lmpiP -lbfd -liberty -lintl

For example, the new mpiP link line becomes

  $ mpcc -g -O mpi-foo.c -o mpi-foo.exe -L${mpiP_root}/lib -lmpiP -lbfd
-liberty -lintl

from

  $ mpcc -O mpi-foo.c -o mpi-foo.exe

Make sure the mpiP library appears before the MPI library on your link
line. The three libraries (-lbfd -liberty -lintl) provide support for
decoding the symbol information; they are part of GNU binutils.

Run your application. You can verify that mpiP is working by identifying
the header and trailer in standard out...

   0:mpiP:
   0:mpiP: mpiP V2.5 (Build Aug 28 2001/11:55:57)
   0:mpiP: Direct questions and errors to Jeffrey Vetter <vetter3@llnl.gov>
   0:mpiP:
   0:mpiP:
   0:mpiP: found 21557 symbols in file [./9-test-mpip-time.exe]
   0:mpiP:
   0:mpiP: Storing mpiP output in [./9-test-mpip-time.exe.4.37266.mpiP].
   0:mpiP:

By default, the output file is written to the current directory of the
application. mpiP files are always much smaller than trace files, so
writing them to this directory is safe.


Supported Platforms

The 2.5 release of mpiP supports Linux, Tru64, and AIX. Please contact
us with bug reports or questions regarding these platforms. Note that
mpiP will not work with MPICH 1.1 nor with the IBM signal-based MPI
library (libmpi.a, as opposed to the thread-based implementation
libmpi_r.a). 


Building mpiP

Currently, mpiP requires a compatible GNU binutils
<http://sources.redhat.com/binutils> installation. The binutils include
and lib directories must be specified with the --with-include and
--with-ldflags configure flags. It is very likely that the compilers
will need to be indentified, as well, with the --with-cc, --with-cxx,
and --with-f77 flags. After running configure with the appropriate
arguments, "make" will build the appropriate libraries in the mpiP
directory.


Runtime Configuration of mpiP

mpiP has several configurable parameters that a user can set via an
environment variable: MPIP. The setting for MPIP looks like command-line
parameters: "-t 10 -k 2". Currently, mpiP has several configurable
parameters.

Option   Description Default
-g       Enable mpiP debug mode. disabled
-s n     Set hash table size to <n>. 256
-f dir   Record output file in directory <dir>. .
-k n     Sets callsite stack traceback depth to <n>. 1
-t x     Set print threshold for report, where <x> is the MPI percentage of
           time for each callsite. 0.0
-n       Strip directory names from source code filenames.
-o       Disable profiling at initialization. Application must enable
           profiling with MPI_Pcontrol().

For example, to set the callsite stack walking depth to 2 and the report
print threshold to 10%, you simply need to define the mpiP string in
your environment:

  $ export MPIP="-t 10.0 -k 2"

mpiP prints a message at initialization if it successfully finds this
MPIP variable.


mpiP Output

Here's some sample output from mpiP with an application that has 4 MPI
calls. I'll break it down by sections below. Here's the experiment
setup. Note that MPIP does not capture information about ALL MPI calls.
Local calls, such as MPI_Comm_size, are omitted from the profiling
library measurement to reduce perturbation and mpiP output.

Here's the test code.

  sleeptime = 10;
  MPI_Init (&argc, &argv);
  MPI_Comm_size (comm, &nprocs);
  MPI_Comm_rank (comm, &rank);
  MPI_Barrier (comm);
  if (rank == 0)
    {
      sleep (sleeptime);        /* slacker! delaying everyone else */
    }
  MPI_Barrier (comm);
  MPI_Finalize ();

The code was compiled with

  $ mpcc -g -g -DAIX 9-test-mpip-time.c -o 9-test-mpip-time.exe  \
       -L.. -L/g/g2/vetter/AIX/lib  -lmpiP -lbfd -liberty -lintl

Environment variables were set as

  $ export MPIP="-t 10.0"

The example was executed on snow like this

  $ ./9-test-mpip-time.exe -procs 4 -nodes 1

This experiment produced an output file

  ./9-test-mpip-time.exe.4.37266.mpiP

that we can now analyze. Header information provides basic
information about your performance experiment.

@ mpiP
@ Command : ./9-test-mpip-time.exe
@ Version                  : 2.5
@ MPIP Build date          : Aug 28 2001, 11:55:57
@ Start time               : 2001 08 28 12:07:18
@ Stop time                : 2001 08 28 12:07:28
@ MPIP env var : -t 10.0
@ Collector Rank           : 0
@ Collector PID            : 37266
@ Final Output Dir         : .
@ MPI Task Assignment      : 0 snow06.llnl.gov
@ MPI Task Assignment      : 1 snow06.llnl.gov
@ MPI Task Assignment      : 2 snow06.llnl.gov
@ MPI Task Assignment      : 3 snow06.llnl.gov

This next section above provides an overview of the application's time
in MPI. Apptime is the wall-clock time from the end of MPI_Init until
the beginning of MPI_Finalize. MPI_Time is the wall-clock time for all
the MPI calls contained within Apptime. MPI% shows the ratio of this
MPI_Time to Apptime. The asterisk (*) is the aggregate line for the
entire application.

---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0         10   0.000243     0.00
   1         10         10    99.92
   2         10         10    99.92
   3         10         10    99.92
   *         40         30    74.94

The callsite section identifies all the MPI callsites within the
application. The first number is the callsite ID for this mpiP file. The
next column shows the type of MPI call (w/o the MPI_ prefix). The name
of the function that contains this MPI call is next, followed by the
filename and line number. Finally, the last column shows the PC, or
program counter for that MPI callsite. Note that the default setting for
callsite stack walk depth is 1. Other settings will enumerate callsites
by the entire stack trace rather than the single callsite alone.

---------------------------------------------------------------------------
@--- Callsites: 2 ---------------------------------------------------------
---------------------------------------------------------------------------
 ID Lev File            Line Parent_Funct         MPI_Call
  1   0 9-test-mpip-time.c   47 .main                          Barrier
  2   0 9-test-mpip-time.c   56 .main                          Barrier

The aggregate time section is a very quick overview of the top twenty
MPI callsites that consume the most aggregate time in your application.
Call identifies the type of MPI function. Site provides the callsite ID
(as listed in the callsite section). Time is the aggregate time for that
callsite in milliseconds. The final two columns show the ratio of that
aggregate time to the total application time and to the total MPI time,
respectively.

---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site      Time     App%    MPI%
Barrier                 2      3e+04   74.94  100.00
Barrier                 1      0.547    0.00    0.00

The next section is similar to the aggregate time section, although it
reports on the top 20 callsite for total sent message sizes. For example:

--------------------------------------------------------------------------- 
@--- Aggregate Sent Message Size (top twenty, descending, bytes) ---------- 
--------------------------------------------------------------------------- 
Call                 Site      Count      Total       Avrg   MPI% 
Send                    7        320   1.92e+06      6e+03  99.96 
Bcast                   1         12        336         28   0.02 

The final sections are the ad nauseum listing of the statistics for each
callsite across all tasks, followed by an aggregate line (indicated by
an asterisk in the Rank column). The first section is for operation time
followed by the a section for message sizes.

---------------------------------------------------------------------------
@--- Callsite statistics (all, milliseconds): 8 ---------------------------
---------------------------------------------------------------------------
Name              Site Rank  Count      Max     Mean      Min   App%   MPI%
Barrier              1    0      1    0.107    0.107    0.107   0.00  44.03
Barrier              1    *      4    0.174    0.137    0.107   0.00   0.00

Barrier              2    0      1    0.136    0.136    0.136   0.00  55.97
Barrier              2    1      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    2      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    3      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    *      4    1e+04  7.5e+03    0.136  74.94 100.00

Remember that we configured MPIP to not print lines where MPI% was less
than 10%. All aggregate lines are printed regardless of the
configuration settings.

Column   Description
Name     Name of the MPI function at that callsite.
Site     Callsite ID as listed in the callsite section above.
Rank     Task rank in MPI_COMM_WORLD.
Count    Number of times this call was executed.
Max      Maximum wall-clock time for one call.
Mean     Arithmetic mean of the wall-clock time for one call.
Min      Minimum wall-clock time for one call.
App%     Ratio of time for this call to the overall application time for
           each task.
MPI%     Ratio of time for this call to the overall MPI time for each task.

The aggregate result for each call has the same measurement meaning;
however, the statistics are gathered across all tasks and compared with
the aggregate application and MPI times.

The section for sent message sizes has a similar format.

--------------------------------------------------------------------------- 
@--- Callsite statistics (all, sent bytes) -------------------------------- 
--------------------------------------------------------------------------- 
Name              Site Rank   Count       Max      Mean       Min       Sum 
Send                 5    0      80      6000      6000      6000   4.8e+05 
Send                 5    1      80      6000      6000      6000   4.8e+05 
Send                 5    2      80      6000      6000      6000   4.8e+05 
Send                 5    3      80      6000      6000      6000   4.8e+05 
Send                 5    *     320      6000      6000      6000  1.92e+06 


Column   Description
Name     Name of the MPI function at that callsite.
Site     Callsite ID as listed in the callsite section above.
Rank     Task rank in MPI_COMM_WORLD.
Count    Number of times this call was executed.
Max      Maximum sent message size in bytes for one call.
Mean     Arithmetic mean of the sent message sizes in bytes for one call.
Min      Minimum sent message size in bytes for one call.
Sum      Total of all message sizes for this operation and callsite.


Controlling the Scope of MPIP Profiling in your Application

In MPIP, you can limit the scope of profiling measurements to specific
regions of your code using the MPI_Pcontrol(int level) subroutine. A
value of zero disables MPIP profiling while any nonzero value enables
profiling. To disable profiling initially at MPI_Init, use the -o
configuration option. MPIP will only record information about MPI
commands encountered between activation and deactivation. There is no
limit to the number to times that an application can activate profiling
during execution.

For example, in your application you can capture the MPI activity for
timestep 5 only using Pcontrol. Remeber to set the MPIP environment
variable to include '-o' when using this feature.

for(i=1; i < 10; i++)
{
  switch(i)
  {
    case 5:
      MPI_Pcontrol(1);
      break;
    case 6:
      MPI_Pcontrol(0);
      break;
    default:
      break;
  }
  /* ... compute and communicate for one timestep ... */
}


Caveats

    * If mpiP has problems with the source code translation, you might
      be able to decode the program counters on LLNL systems with some
      of the following techniques. You can use instmap, addr2line, or
      look at the assembler code itself.
    * Compiler transformations like loop unrolling can sometimes make
      one source code line appear as many different PCs. You can verify
      this by looking at the assembler. In my experience, both instmap
      and addr2line do a pretty good job of mapping these transformed
      PCs into a filename and line number.
          o  instmap - an IBM utility
          o  addr2line - a gnu tool
          o  look at the assembler listing, or with GNU's objdump (-d -S)
          o  use Totalview or gdb to translate the PC
    * There are known incompatibilities with certain binutils versions
      and recent versions of the IBM compilers. As of this release, a
      fix has not been incorporated into binutils, however, using the
      "-bnoobjreorder" option is a valid work-around.
    * Issues when stack walking optimized applications:
          o Applications compiled with gcc may return incorrect parent
            functions, however, the file and line number information may
            be correct.
          o Applications compiled with the Intel compiler may not be
            able to identify parent stack frames.
    * If you are calling MPI functions from within dynamically loaded
      objects, you may need to recompile the library as a shared object.
    * The mpiP library currently will not compile with g++ >= 2.96.
    * The mpiP library currently will not link with the signal-based IBM
      mpi library.


MPI Routines Profiled with MPIP

    MPI_Allgather
    MPI_Allgatherv
    MPI_Allreduce
    MPI_Alltoall
    MPI_Alltoallv
    MPI_Attr_delete
    MPI_Attr_get
    MPI_Attr_put
    MPI_Barrier
    MPI_Bcast
    MPI_Bsend
    MPI_Bsend_init
    MPI_Buffer_attach
    MPI_Buffer_detach
    MPI_Cancel
    MPI_Cart_coords
    MPI_Cart_create
    MPI_Cart_get
    MPI_Cart_map
    MPI_Cart_rank
    MPI_Cart_shift
    MPI_Cart_sub
    MPI_Cartdim_get
    MPI_Comm_create
    MPI_Comm_dup
    MPI_Comm_group
    MPI_Comm_remote_group
    MPI_Comm_remote_size
    MPI_Comm_split
    MPI_Comm_test_inter
    MPI_Dims_create
    MPI_Error_class
    MPI_Gather
    MPI_Gatherv
    MPI_Graph_create
    MPI_Graph_get
    MPI_Graph_map
    MPI_Graph_neighbors
    MPI_Graph_neighbors_count
    MPI_Graphdims_get
    MPI_Group_compare
    MPI_Group_difference
    MPI_Group_excl
    MPI_Group_free
    MPI_Group_incl
    MPI_Group_intersection
    MPI_Group_translate_ranks
    MPI_Group_union
    MPI_Ibsend
    MPI_Intercomm_create
    MPI_Intercomm_merge
    MPI_Iprobe
    MPI_Irecv
    MPI_Irsend
    MPI_Isend
    MPI_Issend
    MPI_Keyval_create
    MPI_Keyval_free
    MPI_Pack
    MPI_Probe
    MPI_Recv
    MPI_Recv_init
    MPI_Reduce
    MPI_Reduce_scatter
    MPI_Request_free
    MPI_Rsend
    MPI_Rsend_init
    MPI_Scan
    MPI_Scatter
    MPI_Scatterv
    MPI_Send
    MPI_Send_init
    MPI_Sendrecv
    MPI_Sendrecv_replace
    MPI_Ssend
    MPI_Ssend_init
    MPI_Start
    MPI_Startall
    MPI_Test
    MPI_Testall
    MPI_Testany
    MPI_Testsome
    MPI_Topo_test
    MPI_Type_commit
    MPI_Type_free
    MPI_Type_get_contents
    MPI_Type_get_envelope
    MPI_Unpack
    MPI_Wait
    MPI_Waitall
    MPI_Waitany
    MPI_Waitsome


MPI Routines for which MPIP gathers sent message size data

    MPI_Allgather
    MPI_Allgatherv
    MPI_Allreduce
    MPI_Alltoall
    MPI_Bcast
    MPI_Bsend
    MPI_Gather
    MPI_Gatherv
    MPI_Ibsend
    MPI_Irsend
    MPI_Isend
    MPI_Issend
    MPI_Reduce
    MPI_Rsend
    MPI_Scan
    MPI_Scatter
    MPI_Send
    MPI_Sendrecv
    MPI_Sendrecv_replace
    MPI_Ssend 

------------------------------------------------------------------------

For further information contact: vetter3@llnl.gov
Jeffrey Vetter, (925) 424-6284.

Last modified on August 15th, 2003.

UCRL-CODE-2002-020, Version 2.

LLNL Disclaimers <http://www.llnl.gov/disclaimer.html>

