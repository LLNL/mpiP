<html><head><style type="text/css">
<!--
body,p,h1,h2,h3,th,td,ol,ul,li {
	font-family: Verdana, Arial, Helvetica, sans-serif;
	color: #000000;
}
.privacyfoter {
	font-family: Verdana, Arial, Helvetica, sans-serif;
	font-size: 10px;
}
.commandline {
	font-family: "Courier New", Courier, mono;
}
-->
</style></head><body><p><a href="http://www.llnl.gov/disclaimer.html" class="privacyfoter">Privacy &amp; Legal Notice</a></p>
<h1 align="center"><a name="top">mpiP: Lightweight, Scalable MPI Profiling</a></h1>
<p align="center">Version 2.5 <br>
18 August 2003 <br>
<br>
Jeffrey Vetter <br>
<a href="mailto:vetter3@llnl.gov">vetter3@llnl.gov</a><br>
<br>
Chris Chambreau <br>
<a href="mailto:chcham@llnl.gov">chcham@llnl.gov</a></p>
<hr width="100%">
<h2>Contents</h2>
<ul>
<li><a href="#Introduction">Introduction</a><ul>
<li><a href="#Downloading">Downloading</a></li>
<li><a href="#Contributing">Contributing</a></li>
<li><a href="#New_Features">New Features</a></li></ul></li>
<li><a href="#Using">Using mpiP (summary)</a><ul>
<li><a href="#Supported">Supported Platforms</a></li>
</ul>
</li>
<li><a href="#Building">Building mpiP</a><ul>
<li><a href="#Linking_Examples">Linking Examples</a></li>
</ul>
</li>
<li><a href="#Runtime_Configuration">Run-time Configuration</a></li>
<li><a href="#mpiP_Output">mpiP Output</a></li>
<li><a href="#Controlling_Scope">Controlling mpiP Profiling Scope</a></li>
<li><a href="#Caveats">Caveats</a></li>
<li><a href="#Profiled_Routines">List of Profiled Routines</a></li>
<li><a href="#Message_Size_Routines">List of Routine With Collected Sent Message 
Size Information</a></li>
</ul>
<hr width="75%">
<h2><a name="Introduction"></a>Introduction</h2>
<p>mpiP is a lightweight profiling library for MPI applications. Because it only 
collects statistical information about MPI functions, mpiP generates considerably 
less overhead and much less data than tracing tools. All the information captured 
by mpiP is task-local. It only uses communication at the end of the application 
experiment to merge results from all of the tasks into one output file.
</p><p> We have tested mpiP on a variety of C/C++/Fortran applications from 2 to 1536 
tasks. Please send your comments, questions, and ideas for enhancements to <a href="mailto:vetter3@llnl.gov">vetter3@llnl.gov</a>.
</p><p> To learn more about performance analysis with mpiP, see Vetter, J.S. and M.O. 
McCracken, "<a href="http://www.llnl.gov/CASC/people/vetter/vetter_pubs.html">Statistical 
Scalability Analysis of Communication Operations in Distributed Applications</a>," 
Proc. ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming (PPOPP), 
2001.
</p><h3><a name="Downloading">Downloading</a></h3>
<p>You may download the current version of mpiP from <a href="http://www.llnl.gov/CASC/mpip/download/">http://www.llnl.gov/CASC/mpip/download/</a></p>
<h3><a name="Contributing">Contributing</a></h3>
<p>We are constantly improving mpiP. Bug fixes and ports to new platforms are 
always welcome. Many thanks to the following contributors:</p>
<ul>
<li>Michael McCracken (UCSD)</li>
<li>Curt Janssen (Sandia National Laboratories)</li>
<li>Mike Campbell (UIUC)</li>
</ul>
<h3><a name="New_Features">New Features with v2.5</a></h3>
<p>Release v2.5 corrects some outstanding issues and provides the following new 
features:</p>
<ul>
<li>Message Size Reporting</li>
<li>C++ name demangling</li>
<li>Long function and file name support</li>
</ul>
<p class="privacyfoter"><a href="#top">Top</a></p>
<hr width="75%">
<h2><a name="Using">Using mpiP</a></h2>
<p>Using mpiP is very simple. Because it gathers MPI information through the MPI 
profiling layer, mpiP is a link-time library. That is, you don't have to recompile 
your application to use mpiP. Note that you might have to recompile to include 
the '-g' option. This is important if you want mpiP to decode the PC to a source 
code filename and line number automatically. mpiP will work without <span class="commandline">-g</span>, 
but mileage may vary.</p>
<p> To compile a simple program on LLNL AIX, you need to add the following libraries 
to your compile/link line:</p>
<blockquote>
<p><span class="commandline">-L${mpiP_root}/lib -lmpiP -lbfd -liberty -lintl</span></p>
</blockquote>
<p>For example, the new mpiP link line becomes</p>
<blockquote>
<p class="commandline">$ mpcc -g -O mpi-foo.c -o mpi-foo.exe -L${mpiP_root}/lib
-lmpiP -lbfd -liberty -lintl</p>
</blockquote>
<p>from</p>
<blockquote>
<p class="commandline">$ mpcc -O mpi-foo.c -o mpi-foo.exe</p>
</blockquote>
<p> Make sure the mpiP library appears before the MPI library on your link line. 
The three libraries (<span class="commandline">-lbfd -liberty -lintl</span>) provide 
support for decoding the symbol information; they are part of GNU binutils.</p>
<p> Run your application. You can verify that mpiP is working by identifying the 
header and trailer in standard out.</p>
<blockquote>
<p class="commandline">0:mpiP:<br>
0:mpiP: mpiP V2.5 (Build Aug 28 2001/11:55:57)<br>
0:mpiP: Direct questions and errors to Jeffrey Vetter &lt;vetter3@llnl.gov&gt;<br>
0:mpiP:<br>
0:mpiP:<br>
0:mpiP: found 21557 symbols in file [./9-test-mpip-time.exe]<br>
0:mpiP:<br>
0:mpiP: Storing mpiP output in [./9-test-mpip-time.exe.4.37266.mpiP].<br>
0:mpiP:</p>
</blockquote>
<p> By default, the output file is written to the current directory of the application. 
mpiP files are always much smaller than trace files, so writing them to this directory 
is safe.</p>
<h3><a name="Supported">Supported Platforms</a></h3>
<p>The 2.5 release of mpiP supports Linux, Tru64, and AIX. Please contact us with 
bug reports or questions regarding these platforms. Note that mpiP will not work 
with MPICH 1.1 nor with the IBM signal-based MPI library (libmpi.a, as opposed 
to the thread-based implementation libmpi_r.a). The following table indicates 
platforms where mpiP was succesfully run and any requirements for that platform.</p>
<table width="100%" border="1" cellspacing="0" cellpadding="3">
<tbody><tr> 
<th>Platform</th>
<th>OS</th>
<th>Compiler</th>
<th>MPI</th>
<th>binutils</th>
<th>Requirements</th>
</tr>
<tr> 
<td>IBM Power-3/4</td>
<td>AIX 5.1</td>
<td>VA C 5.0/<br>
VA Fortran 7.1</td>
<td>PE 3.2</td>
<td>post-7/22/03 snapshot</td>
<td>Newer compiler versions require <span class="commandline">-bnoobjreorder</span> 
linker flag. Also, binutils snapshot after July 22, 2003 corrects storage class 
messages.</td>
</tr>
<tr> 
<td>IA32-Linux</td>
<td>2.4.18 Kernel</td>
<td>gcc 2.96,<br>
Intel 6.0</td>
<td>Quadrics MPI 1.24-8, MPICH 1.2 2.14</td>
<td>2.14</td>
<td>&nbsp;</td>
</tr>
<tr> 
<td>Alpha EV67</td>
<td>Tru64 5.1</td>
<td>Compaq C 6.4/<br>
Fortran 5.5</td>
<td>Quadrics RMS 2.5 2.14</td>
<td>2.14</td>
<td>&nbsp;</td>
</tr>
</tbody></table>
<p class="privacyfoter"><a href="#top">Top</a></p>
<hr width="75%">
<h2><a name="Building">Building mpiP</a></h2>
<p>Currently, mpiP requires a compatible GNU <a href="http://sources.redhat.com/binutils">binutils</a> 
installation. The binutils include and lib directories must be specified with 
the <span class="commandline">--with-include</span> and <span class="commandline">--with-ldflags</span> 
configure flags. It is very likely that the compilers will need to be indentified 
as well, with the <span class="commandline">--with-cc</span>, <span class="commandline">--with-cxx</span>, 
and <span class="commandline">--with-f77</span> flags. After running configure 
with the appropriate arguments, "make" will build the appropriate libraries 
in the mpiP directory. </p>
<h3><a name="Linking_Examples">Example Link Commands for LLNL Machines</a></h3>
<table width="100%" border="1" cellspacing="0" cellpadding="3">
<tbody><tr> 
<th>OS</th>
<th>Compiler</th>
<th>Language</th>
<th>Example Link Command</th>
</tr>
<tr> 
<td rowspan="3" align="center">AIX</td>
<td rowspan="3" align="center">Visual Age</td>
<td>C</td>
<td class="commandline">mpxlc -g -bnoobjreorder 1-hot-potato.c -o 1-hot-potato.exe
-L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td>C++</td>
<td class="commandline">mpCC_r -g -bnoobjreorder 4-demangle.C -o 4-demangle.exe
-L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td>Fortran</td>
<td class="commandline">mpxlf -g -bnoobjreorder sweep-ops.f -o sweep-ops.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td rowspan="6" align="center">Linux</td>
<td rowspan="3" align="center">Intel</td>
<td>C</td>
<td class="commandline">mpiicc -g 1-hot-potato.c -o 1-hot-potato.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td>C++</td>
<td class="commandline">mpiicc -g 4-demangle.C -o 4-demangle.exe -L/usr/local/tools/mpiP/lib
-lmpiPdmg -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td>Fortran</td>
<td class="commandline">mpiifc -g sweep-ops.f -o sweep-ops.exe -L/usr/local/tools/mpiP/lib
-lmpiPifc -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td rowspan="3" align="center">GNU</td>
<td>C</td>
<td class="commandline">mpicc -g 1-hot-potato.c -o 1-hot-potato.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td>C++</td>
<td class="commandline">mpiCC -g 4-demangle.C -o 4-demangle.exe -L/usr/local/tools/mpiP/lib
-lmpiPdmg -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td>Fortran</td>
<td class="commandline">mpif77 -g sweep-ops.f -o sweep-ops.exe -L/usr/local/tools/mpiP/lib
-lmpiPg77 -lbfd -liberty -lintl</td>
</tr>
<tr> 
<td rowspan="3" align="center">Tru64</td>
<td rowspan="3" align="center">Compaq</td>
<td>C</td>
<td class="commandline">cc -g -I/usr/lib/mpi/include 1-hot-potato.c -o 1-hot-potato.exe
-L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lmpi -lpmpi -lexc</td>
</tr>
<tr> 
<td>C++</td>
<td class="commandline">cxx -g 4-demangle.C -o 4-demangle.exe -L/usr/local/tools/mpiP/lib
-lmpiPdmg -lbfd -liberty -lintl -lmpi -lpmpi -lexc -lmld</td>
</tr>
<tr> 
<td>Fortran</td>
<td class="commandline">f77 -g -I/usr/lib/mpi/include sweep-ops.f -o sweep-ops.exe
-L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lmpi -lpmpi -lexc</td>
</tr>
</tbody></table>
<p><b>Note</b>:</p>
<ul>
<li>On Linux, the mpiP Fortran libraries have unique names: libmpiPifc.a for the 
Intel compiler and libmpiPg77.a for the GNU compiler. </li>
<li>Demangling support is implemented in the library libmpiPdmg.a. This library 
will be built if <span class="commandline">--enable-demangling=[GNU|IBM|Compaq]</span> 
is provided to the configure script. Use GNU for the Intel compiler. </li>
<li>On Tru64, the exception library flag (<span class="commandline">-lexc</span>) 
must be added to the link command. </li>
<li>Source lookup for callsites may fail with certain versions of binutils. If 
you are running into trouble, you may want to download a recent snapshot from 
<a href="ftp://sources.redhat.com/pub/binutils/snapshots">ftp://sources.redhat.com/pub/binutils/snapshots</a>.</li>
</ul>
<p class="privacyfoter"><a href="#top">Top</a></p>
<hr width="75%">
<h2><a name="Runtime_Configuration">Run-time Configuration of mpiP</a></h2>
<p>mpiP has several configurable parameters that a user can set via the environment 
variable MPIP. The setting for MPIP looks like command-line parameters: "-t 
10 -k 2". Currently, mpiP has several configurable parameters.</p>
<table width="90%" border="1" cellspacing="0" cellpadding="3">
<tbody><tr> 
<th>Option</th>
<th>Description</th>
<th>Default</th>
</tr>
<tr> 
<td class="commandline">-g</td>
<td>Enable mpiP debug mode. </td>
<td align="center">disabled</td>
</tr>
<tr> 
<td class="commandline">-s n</td>
<td>Set hash table size to &lt;n&gt;.</td>
<td align="center">256</td>
</tr>
<tr> 
<td class="commandline">-f dir</td>
<td>Record output file in directory &lt;dir&gt;.</td>
<td align="center">.</td>
</tr>
<tr> 
<td class="commandline">-k n</td>
<td>Sets callsite stack traceback depth to &lt;n&gt;.</td>
<td align="center">1</td>
</tr>
<tr> 
<td class="commandline">-t x</td>
<td>Set print threshold for report, where &lt;x&gt; is the MPI percentage of time 
for each callsite.</td>
<td align="center">0.0</td>
</tr>
<tr>
<td class="commandline">-o</td>
<td>Disable profiling at initialization. Application must enable profiling with
MPI_Pcontrol().</td>
<td align="center">&nbsp;</td>
</tr>
<tr>
<td class="commandline">-n</td>
<td>Do not truncate full pathname of filename in callsites.</td>
<td align="center">&nbsp;</td>
</tr></tbody></table>
<p>For example, to set the callsite stack walking depth to 2 and the report print 
threshold to 10%, you simply need to define the mpiP string in your environment:</p>
<p class="commandline">$ export MPIP="-t 10.0 -k 2"</p>
<p> mpiP prints a message at initialization if it successfully finds this MPIP 
variable.</p>
<p class="privacyfoter"><a href="#top">Top</a></p>
<hr width="75%">
<h2><a name="mpiP_Output">mpiP Output</a></h2>
<p>Here is some sample output from mpiP with an application that has 4 MPI calls. 
It is broken down by sections below. Here also is the experiment setup. <b>Note 
that MPIP does not capture information about ALL MPI calls</b><i>.</i> Local calls, 
such as <span class="commandline">MPI_Comm_size</span>, are omitted from the profiling 
library measurement to reduce perturbation and mpiP output.</p>
<p><b>The test code</b>:</p>
<pre>  sleeptime = 10; 
  MPI_Init (&amp;argc, &amp;argv); 
  MPI_Comm_size (comm, &amp;nprocs); 
  MPI_Comm_rank (comm, &amp;rank); 
  MPI_Barrier (comm); 
  if (rank == 0) 
    { 
      sleep (sleeptime);        /* slacker! delaying everyone else */ 
    } 
  MPI_Barrier (comm); 
  MPI_Finalize ();
</pre>
<p><b>The code was compiled with</b>:</p>
<pre>  $ mpcc -g -g -DAIX 9-test-mpip-time.c -o 9-test-mpip-time.exe  \
       -L.. -L/g/g2/vetter/AIX/lib  -lmpiP -lbfd -liberty -lintl
</pre>
<p><b>Environment variables were set as</b>:</p>
<pre>  $ export MPIP="-t 10.0"</pre>
<p><b>The example was executed on Snow like this</b>:</p>
<pre>  $ ./9-test-mpip-time.exe -procs 4 -nodes 1</pre>
<p><b>This experiment produced an output file that we can now analyze</b>:</p>
<pre>  ./9-test-mpip-time.exe.4.37266.mpiP</pre>
<p>Header information provides basic information about your performance experiment.</p>
<pre>@ mpiP
@ Command : ./9-test-mpip-time.exe
@ Version                  : 2.5
@ MPIP Build date          : Aug 28 2001, 11:55:57
@ Start time               : 2001 08 28 12:07:18
@ Stop time                : 2001 08 28 12:07:28
@ MPIP env var : -t 10.0
@ Collector Rank           : 0
@ Collector PID            : 37266
@ Final Output Dir         : .
@ MPI Task Assignment      : 0 snow06.llnl.gov
@ MPI Task Assignment      : 1 snow06.llnl.gov
@ MPI Task Assignment      : 2 snow06.llnl.gov
@ MPI Task Assignment      : 3 snow06.llnl.gov</pre>
<p>This next section provides an overview of the application's time in MPI. Apptime 
is the wall-clock time from the end of <span class="commandline">MPI_Init</span> 
until the beginning of <span class="commandline">MPI_Finalize</span>. MPI_Time 
is the wall-clock time for all the MPI calls contained within Apptime. MPI% shows 
the ratio of this MPI_Time to Apptime. The asterisk (*) is the aggregate line 
for the entire application.</p>
<pre>---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime    MPI%
   0         10   0.000243    0.00
   1         10         10   99.92
   2         10         10   99.92
   3         10         10   99.92
   *         40         30   74.94</pre>
<p> The callsite section identifies all the MPI callsites within the application. 
The first number is the callsite ID for this mpiP file. The next column shows 
the type of MPI call (w/o the MPI_ prefix). The name of the function that contains 
this MPI call is next, followed by the file name and line number. Finally, the 
last column shows the PC, or program counter, for that MPI callsite. Note that 
the default setting for callsite stack walk depth is 1. Other settings will enumerate 
callsites by the entire stack trace rather than the single callsite alone.</p>
<pre>---------------------------------------------------------------------------
@--- Callsites: 2 ---------------------------------------------------------
---------------------------------------------------------------------------
 ID Lev File            Line Parent_Funct         MPI_Call
  1   0 9-test-mpip-time.c   47 .main                          Barrier
  2   0 9-test-mpip-time.c   56 .main                          Barrier</pre>
<p>The aggregate time section is a very quick overview of the top twenty MPI 
callsites that consume the most aggregate time in your application. Call identifies 
the type of MPI function. Site provides the callsite ID (as listed in the callsite 
section). Time is the aggregate time for that callsite in milliseconds. The final 
two columns show the ratio of that aggregate time to the total application time 
and to the total MPI time, respectively.</p>
<pre>---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site      Time     App%     MPI%
Barrier                 2      3e+04   74.94   100.00 
Barrier                 1      0.547    0.00     0.00</pre>
<p>The next section is similar to the aggregate time section, although it reports 
on the top 20 callsites for total sent message sizes. For example:</p>
<pre>--------------------------------------------------------------------------- 
@--- Aggregate Sent Message Size (top twenty, descending, bytes) ---------- 
--------------------------------------------------------------------------- 
Call                 Site      Count      Total    Avrg      MPI% 
Send                    7        320   1.92e+06   6e+03     99.96 
Bcast                   1         12        336      28      0.02</pre>
<p>The final sections are the ad nauseum listing of the statistics for each callsite 
across all tasks, followed by an aggregate line (indicated by an asterisk in the 
Rank column). The first section is for operation time followed by the section 
for message sizes.</p>
<pre>---------------------------------------------------------------------------
@--- Callsite statistics (all, milliseconds): 8 ---------------------------
---------------------------------------------------------------------------
Name              Site Rank  Count      Max     Mean      Min   App%   MPI%
Barrier              1    0      1    0.107    0.107    0.107   0.00  44.03
Barrier              1    *      4    0.174    0.137    0.107   0.00   0.00

Barrier              2    0      1    0.136    0.136    0.136   0.00  55.97
Barrier              2    1      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    2      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    3      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    *      4    1e+04  7.5e+03    0.136  74.94 100.00</pre>
<p>Remember that we configured MPIP to not print lines where MPI% was less than 
10%. All aggregate lines are printed regardless of the configuration settings.</p>
<table width="70%" border="1" cellspacing="0" cellpadding="3">
<tbody><tr> 
<th>Column</th>
<th>Description</th>
</tr>
<tr> 
<td>Name</td>
<td>Name of the MPI function at that callsite.</td>
</tr>
<tr> 
<td>Site</td>
<td>Callsite ID as listed in the callsite section above.</td>
</tr>
<tr> 
<td>Rank</td>
<td>Task rank in MPI_COMM_WORLD.</td>
</tr>
<tr> 
<td>Count</td>
<td>Number of times this call was executed.</td>
</tr>
<tr> 
<td>Max</td>
<td>Maximum wall-clock time for one call.</td>
</tr>
<tr> 
<td>Mean</td>
<td>Arithmetic mean of the wall-clock time for one call.</td>
</tr>
<tr> 
<td>Min</td>
<td>Minimum wall-clock time for one call.</td>
</tr>
<tr> 
<td>App%</td>
<td>Ratio of time for this call to the overall application time for each task.</td>
</tr>
<tr> 
<td>MPI%</td>
<td>Ratio of time for this call to the overall MPI time for each task.</td>
</tr>
</tbody></table>
<p> The aggregate result for each call has the same measurement meaning; however, 
the statistics are gathered across all tasks and compared with the aggregate application 
and MPI times.</p>
<p> The section for sent message sizes has a similar format.</p>
<pre>--------------------------------------------------------------------------- 
@--- Callsite statistics (all, sent bytes) -------------------------------- 
--------------------------------------------------------------------------- 
Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Site Rank&nbsp;&nbsp; Count&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Max&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mean&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Min&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum 
Send&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp; 4.8e+05 
Send&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp; 4.8e+05 
Send&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp; 4.8e+05 
Send&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp; 4.8e+05 
Send&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; *&nbsp;&nbsp;&nbsp;&nbsp; 320&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6000&nbsp; 1.92e+06</pre>
<table width="70%" border="1" cellspacing="0" cellpadding="3">
<tbody><tr> 
<th>Column</th>
<th>Description</th>
</tr>
<tr> 
<td>Name</td>
<td>Name of the MPI function at that callsite.</td>
</tr>
<tr> 
<td>Site</td>
<td>Callsite ID as listed in the callsite section above.</td>
</tr>
<tr> 
<td>Rank</td>
<td>Task rank in MPI_COMM_WORLD.</td>
</tr>
<tr> 
<td>Count</td>
<td>Number of times this call was executed.</td>
</tr>
<tr> 
<td>Max</td>
<td>Maximum sent message size in bytes for one call.</td>
</tr>
<tr> 
<td>Mean</td>
<td>Arithmetic mean of the sent message sizes in bytes for one call.</td>
</tr>
<tr> 
<td>Min</td>
<td>Minimum sent message size in bytes for one call.</td>
</tr>
<tr> 
<td>Sum</td>
<td>Total of all message sizes for this operation and callsite.</td>
</tr>
</tbody></table>
<p class="privacyfoter"><a href="#top">Top</a></p>
<hr width="75%">
<h2><a name="Controlling_Scope">Controlling the Scope of mpiP Profiling
in your Application</a></h2>
<p>In mpiP, you can limit the scope of profiling measurements to specific regions 
of your code using the <span class="commandline">MPI_Pcontrol(int level)</span> 
subroutine. A value of zero disables mpiP profiling, while any nonzero value enables 
profiling. To disable profiling initially at MPI_Init, use the <span class="commandline">-o</span>
configuration option. mpiP will only record information about MPI commands encountered 
between activation and deactivation. There is no limit to the number to times 
that an application can activate profiling during execution.</p>
<p> For example, in your application you can capture the MPI activity for timestep 
5 only using Pcontrol. Remember to set the mpiP environment variable to include 
<span class="commandline">-o</span> when using this feature.</p>
<pre>for(i=1; i &lt; 10; i++) 
{ 
  switch(i) 
  { 
    case 5: 
      MPI_Pcontrol(1); 
      break; 
    case 6: 
      MPI_Pcontrol(0); 
      break; 
    default: 
      break; 
  } 
  /* ... compute and communicate for one timestep ... */ 
}</pre>
<p class="privacyfoter"><a href="#top">Top</a></p>
<hr width="75%">
<h2><a name="Caveats">Caveats</a></h2>
<ul>
<li>If mpiP has problems with the source code translation, you might be able to 
decode the program counters on LLNL systems with some of the following techniques. 
You can use instmap, addr2line, or look at the assembler code itself.</li>
<li>Compiler transformations like loop unrolling can sometimes make one source 
code line appear as many different PCs. You can verify this by looking at the 
assembler. In my experience, both instmap and addr2line do a pretty good job of 
mapping these transformed PCs into a file name and line number. 
<ul>
<li>instmap&#8212;an IBM utility</li>
<li>addr2line&#8212;a gnu tool</li>
<li>look at the assembler listing, or with GNU's objdump (<span class="commandline">-d 
-S</span>)</li>
<li>use Totalview or gdb to translate the PC</li>
</ul>
</li><li>There are known incompatibilities with certain binutils versions and recent 
versions of the IBM compilers. As of this release, a fix has not been incorporated 
into binutils, however, using the <span class="commandline">-bnoobjreorder</span> 
option is a valid work-around.</li>
<li>Issues when stack walking optimized applications: 
<ul>
<li>Applications compiled with gcc may return incorrect parent functions; however, 
the file and line number information may be correct.</li>
<li>Applications compiled with the Intel compiler may not be able to identify 
parent stack frames.</li>
</ul>
</li><li>If you are calling MPI functions from within dynamically loaded objects, you 
may need to recompile the library as a shared object.</li>
<li>The mpiP library currently will not compile with g++ &gt;= 2.96.</li>
<li>The mpiP library currently will not link with the signal-based IBM mpi library.</li>
</ul>
<p class="privacyfoter"><a href="#top">Top</a></p>
<blockquote>
<hr width="75%">
</blockquote>
<h2><a name="Profiled_Routines">MPI Routines Profiled with mpiP</a></h2>
<p>MPI_Allgather <br>
MPI_Allgatherv <br>
MPI_Allreduce <br>
MPI_Alltoall <br>
MPI_Alltoallv <br>
MPI_Attr_delete <br>
MPI_Attr_get <br>
MPI_Attr_put <br>
MPI_Barrier <br>
MPI_Bcast <br>
MPI_Bsend <br>
MPI_Bsend_init <br>
MPI_Buffer_attach <br>
MPI_Buffer_detach <br>
MPI_Cancel <br>
MPI_Cart_coords <br>
MPI_Cart_create <br>
MPI_Cart_get <br>
MPI_Cart_map <br>
MPI_Cart_rank <br>
MPI_Cart_shift <br>
MPI_Cart_sub <br>
MPI_Cartdim_get <br>
MPI_Comm_create <br>
MPI_Comm_dup <br>
MPI_Comm_group <br>
MPI_Comm_remote_group <br>
MPI_Comm_remote_size <br>
MPI_Comm_split <br>
MPI_Comm_test_inter <br>
MPI_Dims_create <br>
MPI_Error_class <br>
MPI_Gather <br>
MPI_Gatherv <br>
MPI_Graph_create <br>
MPI_Graph_get <br>
MPI_Graph_map <br>
MPI_Graph_neighbors <br>
MPI_Graph_neighbors_count <br>
MPI_Graphdims_get <br>
MPI_Group_compare <br>
MPI_Group_difference <br>
MPI_Group_excl <br>
MPI_Group_free <br>
MPI_Group_incl <br>
MPI_Group_intersection <br>
MPI_Group_translate_ranks <br>
MPI_Group_union <br>
MPI_Ibsend <br>
MPI_Intercomm_create <br>
MPI_Intercomm_merge <br>
MPI_Iprobe <br>
MPI_Irecv <br>
MPI_Irsend <br>
MPI_Isend <br>
MPI_Issend <br>
MPI_Keyval_create <br>
MPI_Keyval_free <br>
MPI_Pack <br>
MPI_Probe <br>
MPI_Recv <br>
MPI_Recv_init <br>
MPI_Reduce <br>
MPI_Reduce_scatter <br>
MPI_Request_free <br>
MPI_Rsend <br>
MPI_Rsend_init <br>
MPI_Scan <br>
MPI_Scatter <br>
MPI_Scatterv <br>
MPI_Send <br>
MPI_Send_init <br>
MPI_Sendrecv <br>
MPI_Sendrecv_replace <br>
MPI_Ssend <br>
MPI_Ssend_init <br>
MPI_Start <br>
MPI_Startall <br>
MPI_Test <br>
MPI_Testall <br>
MPI_Testany <br>
MPI_Testsome <br>
MPI_Topo_test <br>
MPI_Type_commit <br>
MPI_Type_free <br>
MPI_Type_get_contents <br>
MPI_Type_get_envelope <br>
MPI_Unpack <br>
MPI_Wait <br>
MPI_Waitall <br>
MPI_Waitany <br>
MPI_Waitsome</p>
<p class="privacyfoter"><a href="#top">Top</a></p>
<hr width="75%">
<h2><a name="Message_Size_Routines">MPI Routines For Which mpiP Gathers Sent Message 
Size Data</a></h2>
<p>MPI_Allgather<br>
MPI_Allgatherv<br>
MPI_Allreduce<br>
MPI_Alltoall<br>
MPI_Bcast<br>
MPI_Bsend<br>
MPI_Gather<br>
MPI_Gatherv<br>
MPI_Ibsend<br>
MPI_Irsend<br>
MPI_Isend<br>
MPI_Issend<br>
MPI_Reduce<br>
MPI_Rsend<br>
MPI_Scan<br>
MPI_Scatter<br>
MPI_Send<br>
MPI_Sendrecv <br>
MPI_Sendrecv_replace <br>
MPI_Ssend </p>
<hr>
<p>For further information contact Jeffrey Vetter,<a href="mailto:vetter3@llnl.gov">vetter3@llnl.gov</a>, 
(925) 424-6284.</p>
<p class="privacyfoter"> Last modified on August 26, 2003.<br>
UCRL-CODE-2002-020, Version 2.</p>
</body></html>
