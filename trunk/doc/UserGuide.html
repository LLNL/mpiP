<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>




<style type="text/css">
<!--
body,p,h1,h2,h3,th,td,ol,ul,li {
font-family: Verdana, Arial, Helvetica, sans-serif;
color: #000000;
}
.sectionreturn {
font-family: Verdana, Arial, Helvetica, sans-serif;
font-size: 10px;
}
.tableheader {
font-family: Verdana, Arial, Helvetica, sans-serif;
font-weight: bold;
text-align: center;
}
.pageinfo {
font-family: Verdana, Arial, Helvetica, sans-serif;
font-size: 10px;
}
.commandline {
font-family: "Courier New", Courier, mono;
}
-->
</style><title>mpiP: Lightweight, Scalable MPI Profiling</title></head><body>

<a href="http://sourceforge.net"><img src="http://sflogo.sourceforge.net/sflogo.php?group_id=178874&amp;type=2" alt="SourceForge.net Logo" border="0" height="37" width="125"></a>

<h1 align="center"><a name="top">mpiP: Lightweight, Scalable MPI
Profiling</a></h1>

<p align="center">Version 3.4.1<br>&nbsp; 13 March 2014 <br>

<br>

Jeffrey Vetter <br>

<a href="mailto:jsvetter@users.sourceforge.net">jsvetter@users.sourceforge.net</a><br>

<br>

Chris Chambreau <br>

<a href="mailto:chcham@users.sourceforge.net">chcham@users.sourceforge.net</a></p>

<hr width="100%">
<h2>Contents</h2>

<ul>

  <li><a href="#Introduction">Introduction</a>
    <ul>

      <li><a href="#Downloading">Downloading</a></li>

      <li><a href="#Contributing">Contributing</a></li>

      <li><a href="#New_Features">New Features</a></li>

    </ul>

  </li>

  <li><a href="#Using">Using mpiP (summary)</a>
    <ul>

      <li><a href="#Supported">Supported Platforms</a></li>

    </ul>

  </li>

  <li><a href="#Building">Configuring and Building mpiP</a>
    <ul>

      <li><a href="#Linking_Examples">Linking Examples</a></li>

    </ul>

  </li>

  <li><a href="#Runtime_Configuration">Run-time Configuration</a></li>

  <li><a href="#mpiP_Output">mpiP Output</a></li>

  <ul>

    <li><a href="#Report_Viewers">Report Viewers</a></li>

  </ul>

  <li><a href="#Controlling_Scope">Controlling mpiP Profiling Scope</a></li>

  <li><a href="#Caveats">Caveats</a></li>

  <li><a href="#Profiled_Routines">List of Profiled Routines</a></li>

  <li><a href="#Message_Size_Routines">List of Routines With Collected Sent Message
Size Information</a></li>

  <li><a href="#IO_Routines">List of I/O Routines</a></li>
  <li><a href="#RMA_Routines">List of RMA Routines</a></li>

  <li><a href="#Adding_Calls">How to add MPI calls to profile</a></li>

  <li><a href="UserGuide.html#License">License</a></li>

</ul>

<hr width="75%">
<h2><a name="Introduction"></a>Introduction</h2>

<p>mpiP is a lightweight profiling library for MPI applications.  Because it
only collects statistical information about MPI functions, mpiP generates
considerably less overhead and much less data than tracing tools. All the
information captured by mpiP is task-local. It only uses communication during
report generation, typically at the end of the experiment, to merge results
from all of the tasks into one output file.  </p>

<p> We have tested mpiP on a variety of C/C++/Fortran applications from
2 to 262144 processes, including a 262144-process run on the LLNL
Sequoia BG/Q system. </p>

<p>Please send your comments, questions, and ideas for enhancements
to <a href="mailto:mpip-help@lists.sourceforge.net">mpip-help@lists.sourceforge.net</a>.
To receive mail regarding new mpiP releases, please subscribe to
mpip-announce@lists.sourceforge.net (send e-mail with body "subscribe"
to <a href="mailto:mpip-announce-request@lists.sourceforge.net">mpip-announce-request@lists.sourceforge.net</a>).
Please also consider subscribing to <a href="mailto:mpip-users-request@lists.sourceforge.net">mpip-users@lists.sourceforge.net</a>
to contribute and receive mpiP use and status information.</p>

<p> To learn more about performance analysis with mpiP, see Vetter, J.S. and
M.O.  McCracken, "<a href="http://mpip.sourceforge.net/statistical-scalability-analysis-of.pdf">Statistical
Scalability Analysis of Communication Operations in Distributed
Applications</a>," Proc. ACM SIGPLAN Symp. on Principles and Practice of
Parallel Programming (PPOPP), 2001.  </p>

<h3><a name="Downloading">Downloading</a></h3>

<p>You may download the current version of mpiP from <a href="http://sourceforge.net/projects/mpip">http://sourceforge.net/projects/mpip</a>.</p>

<h3><a name="Contributing">Contributing</a></h3>

<p>We are constantly improving mpiP. Bug fixes and ports to new platforms are
always welcome. Many thanks to the following contributors (chronological
order):</p>

<ul>

  <li>Michael McCracken (UCSD)</li>

  <li>Curt Janssen (Sandia National Laboratories)</li>

  <li>Mike Campbell (UIUC)</li>

  <li>Jim Brandt (Sandia National Laboratories)</li>

  <li>Philip Roth (Oak Ridge National Laboratory)</li>

  <li>Tushar Mohan (SiCortex)</li>

  <li>Philip Mucci (SiCortex)</li>

  <li>Karl Schulz (Texas Advanced Computing Center)</li>
</ul>

<h3><a name="New_Features">New Features with Release 3.4.1</a></h3>

<p>Release v3.4.1 addresses the following issue:
</p>



<ul>
  <li>Added de-activation of shared object source lookup when libbfd is not available.</li>
</ul>
<p>Release v3.4 addresses the following issues:
</p>


<ul>
<li>Compatibility with MPI-3.<br>
</li>
  <li>Histogram reporting for Point-to-point (-p) and Collective (-y) operation message sizes and communicators.</li>
  <li>Added a low-memory-use concise report format, with the ability to
set the default report format and specify report formats at run time.</li>
  <li>Supports MPI call reporting (no call sites) with stack depth (-k) of 0.</li>
  <li>Configure can disable SO lookup functionality.<br>
  </li>
</ul>
<p>Release v3.3 addresses the following issues:
</p>

<ul>

  <li>Support for shared object source lookup with libbfd.</li>
  <li>Improved configuration process for recent versions of binutils and Cray XE6.</li>
  <li>Added "-z" MPIP run time flag to suppress report generation at Finalize.</li>
  <li>Corrected number of stack frames available when using glibc backtrace.</li>
</ul>

<p>Release v3.2.1 addresses the following issue:
</p>

<ul>

  <li>Improved support for SLURM run-time instrumentation.</li>
</ul>

<p>Release v3.2 addresses the following issues:
</p>

<ul>

  <li>Support for MPI RMA functions.</li>
  <li>Support for glibc backtrace.</li>
  <li>Default to MPI_Wtime if platform-specific timers are not found.</li>
</ul>

<p>Release v3.1.2 addresses the following issues:
</p>

<ul>

  <li>Better MPI support for Init_thread, Testany, Testsome, Waitany, and Waitsome.</li>
  <li>Improved support for MIPS64-Linux.</li>
  <li>Added
option to configure for generating weak Fortran symbols in the case of
multiple Fortran mangling schemes in the application object files
(--enable-fortranweak).</li>
  <li>Addressed various outstanding issues (see ChangeLog for more details).</li>

</ul>

<p>Release v3.1.1 addresses the following issues:
</p>

<ul>

  <li>Revert to gettimeofday as default Linux timer.</li>
  <li>MIPS64-Linux stack walking support.</li>
  <li>Catamount dclock timer support.</li>
  <li>Greater install flexibility:
  <ul>
  <li>'install' target only installs lib and doc files.</li>
  <li>'install-api', 'install-bin', 'install-all' targets provide additional install functionality.</li>
  <li>New 'uninstall' target.</li>
  </ul>

</li></ul>

<p>For more information, please see the ChangeLog in the distribution.</p>


<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="Using">Using mpiP</a></h2>

<p>Using mpiP is very simple. Because it gathers MPI information through the
MPI profiling layer, mpiP is a link-time library. That is, you don't have to
recompile your application to use mpiP. Note that you might have to recompile
to include the '-g' option. This is important if you want mpiP to decode the PC
to a source code filename and line number automatically. mpiP will work without
<span class="commandline">-g</span>, but mileage may vary.</p>

<p> To compile a simple program on an LLNL x86_64-linux system where libunwind 
is installed, add the following libraries to your link command:</p>

<blockquote>
  <p><span class="commandline">-L${mpiP_root}/lib
-lmpiP -lm  -lbfd  -liberty -lunwind</span></p>

</blockquote>

<p>For example, the new mpiP link command becomes</p>

<blockquote>
  <p class="commandline">$ mpicc -g 1-hot-potato.o -o 1-hot-potato.exe  -L${mpiP_root}/lib -lmpiP -lm  -lbfd  -liberty -lunwind </p>

</blockquote>

<p>from</p>

<blockquote>
  <p class="commandline">$ mpicc -g 1-hot-potato.o -o 1-hot-potato.exe </p>

</blockquote>

<p> Make sure the mpiP library appears before the MPI library on your link
line.  The libraries (<span class="commandline">-lbfd -liberty </span>)
provide support for decoding the symbol information; they are
part of GNU binutils.</p>

<p> Run your application. You can verify that mpiP is working by identifying
the header and trailer in standard out.</p>

<blockquote>
  <p class="commandline">
</p><pre>mpiP:<br>mpiP: mpiP: mpiP V3.2.0 (Build Mar 10 2010/13:27:39)<br>mpiP: Direct questions and errors to mpip-help@lists.sourceforge.net<br>mpiP:<br>mpiP:<br>mpiP: Storing mpiP output in [./1-hot-potato.exe.2.27872.1.mpiP].<br>mpiP:<br></pre>
<p></p>

</blockquote>

<p> By default, the output file is written to the current directory of the
application.  mpiP files are always much smaller than trace files, so writing
them to this directory is safe.</p>

<h3><a name="Supported">Supported Platforms</a></h3>

<p>mpiP has been tested on several Linux,&nbsp;AIX, UNICOS
and IBM BG systems. Please contact us with bug reports or questions regarding
these platforms.  The following table indicates platforms where mpiP
was succesfully run and any requirements for that platform.</p>

<table style="width: 100%; text-align: left; margin-left: auto; margin-right: auto;" border="1" cellpadding="3" cellspacing="0">

  <tbody>

    <tr>

      <th>Platform</th>

      <th>OS</th>

      <th>Compiler</th>

      <th>MPI</th>

      <th>binutils</th>

      <th>Requirements</th>

    </tr>

    

    

    
    <tr>

      <td>x86_64-Linux</td>

      <td> 2.6.18 CHAOS Kernel</td>

      <td>Intel 9.1<br>

          PGI 7.0</td>

      <td>MVAPICH 0.9.7</td><td style="vertical-align: middle;">2.20.51<br>
      </td>


      <td>Example configure command:<br>
./configure LDFLAGS=-L/usr/lib64 LIBS="-lbfd -liberty"
--enable-collective-report-default --enable-demangling=GNU
--with-cc=mpicc --with-cxx=mpiCC --with-f77=mpif77<br>
  </td>
    </tr>

    

    <tr>
      <td>IBM BG/Q<br>
      </td>
      <td>Driver V1R2M0<br>
      </td>
      <td>IBM XL 12.1<br>
      </td>
      <td style="text-align: center;"><br>
      </td>
      <td>2.21.1<br>
      </td>
      <td>Source code lookup support requires zlib.&nbsp; Example configure command:<br>
CFLAGS=-I/bgsys/drivers/ppcfloor/toolchain/gnu/build-powerpc64-bgq-linux/binutils-2.21.1-build/bfd
-I/bgsys/drivers/ppcfloor/toolchain/gnu/gcc-4.4.6/include
-I/bgsys/drivers/ppcfloor/toolchain/gnu/gdb-7.1/include
LIBS=-L/bgsys/drivers/ppcfloor/toolchain/gnu/build-powerpc64-bgq-linux/binutils-2.21.1-build/bfd
-L/g/g0/chcham/ToolTesting/mpiP/bgqos_0/mpiP-3.3/libz/zlib-1.2.6
CC=mpixlc_r CXX=mpixlcxx_r F77=mpixlf77_r ./configure --enable-getarg<br>
      </td>
    </tr>
<tr>

      <td>Cray XT3/XT4</td>

      <td>Catamount 1.4.32</td>

      <td>
      <p class="MsoPlainText">PGI 6.1-4 C/C/Fortran, from
Cray PrgEnv-pgi module
version 1.4.32<br>

      </p>

      </td>

      <td>
      <p class="MsoPlainText">Cray XT3 Message Passing
Toolkit (MPT) version 1.4.32<br>

      </p>

      </td>

      <td>
      <p class="MsoPlainText">Binutils 2.17, built for
x86_64-unknown-linux-gnu<br>

      </p>

      </td>

      <td>
      <p class="MsoPlainText">--with-cc=cc&nbsp;--with-cxx=CC&nbsp;<br>

--with-f77=ftn <span style=""></span>--with-libs="-lpgf90
-lpgf90_rpm1 -lpgf902 -lpgf90rtl -lpgftnrtl"&nbsp;<br>

      <span style=""></span>--build=x86_64-unknown-linux-gnu&nbsp;<br>

      <span style=""></span>--host=x86_64-cray-catamount&nbsp;<br>

      <span style=""></span>--target=x86_64-cray-catamount<br>

      <span style=""></span>--enable-getarg \<br>

      <span style="">&nbsp;&nbsp;&nbsp; </span>--with-wtime
\<br>

      <span style="">&nbsp;&nbsp;&nbsp; </span>--with-binutils-dir=&lt;path
to binutils-2.17 installation&gt;</p>

      </td>

    </tr>

    <tr>

      <td>Cray X1E</td>

      <td>UNICOS/mp 3.1.16</td>

      <td>Cray Standard C 5.5.0.5, Cray Fortran 5.5.0.5</td>

      <td>Cray Message Passing Toolkit (MPT) 2.4.0.7</td>

      <td>from Cray Open Software module 3.6</td>

      <td>Requires libelf/libdwarf.<br>

Example configure flags: --disable-libunwind --enable-dwarf
--disable-demangle --enable-getarg --with-cc=cc --with-cxx=CC
--with-f77=ftn<br>

Python on an alternative system may be needed to "make wrappers.c", due
to missing socket module.</td>

    </tr>

  </tbody>
</table>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="Building">Configuring and Building mpiP</a></h2>

<h3>Configuring mpiP</h3>

<p>Currently, mpiP requires a compatible GNU <a href="http://sources.redhat.com/binutils">binutils</a> installation for source
lookup and demangling features. Alternatively, <a href="http://directory.fsf.org/libs/misc/libelf.html">libelf</a> and <a href="http://reality.sgiweb.org/davea/dwarf.html">libdwarf</a> can be used for source
lookup. The binutils installation location may need to be specified with either
the <span class="commandline">--with-binutils-dir</span> option or with the
<span class="commandline">--with-include</span> and <span class="commandline">--with-ldflags</span> configure flags. It is likely that
the compilers will need to be indentified as well, with the <span class="commandline">--with-cc</span>, <span class="commandline">--with-cxx</span>, and <span class="commandline">--with-f77</span> flags. Use CFLAGS and FFLAGS variables to
specify compiler options, as in <span class="commandline">CFLAGS="-O3"
./configure</span>.  </p>

<p>There are many configuration options available.  &nbsp;Please use
./configure --help to list all of these options. &nbsp;Additional description
are provided for the following options:</p>

<p></p>

<table style="width: 979px; height: 752px;" border="1" cellpadding="3" cellspacing="0">

  <tbody>

    <tr>

      <th>Flag</th>

      <th>Effect</th>

      <th>Description</th>

    </tr>

    <tr>

      <td class="tableheader" colspan="3" rowspan="1">Reporting Options</td>

    </tr>

    <tr>

      <td>--enable-demangling=[type]</td>

      <td>Specify demangling support.</td>

      <td>If the GNU option is specified, demangling is applied to each symbol
      by default using the libiberty implementation. For the IBM
      option, demangling support is implemented in the library libmpiPdmg.a.
      Use GNU for the Intel compiler.</td>

    </tr>

    <tr>

      <td>--disable-mpi-io</td>

      <td>Disable MPI-I/O reporting.</td>

      <td>Useful for generating an mpiP library without MPI I/O for MPI
      implementations such as Quadrics MPI that has a separate MPI I/O
      library.</td>

    </tr>

    <tr>

      <td>--enable-collective-report-default</td>

      <td>Report data is aggregated on a per-callsite basis</td>

      <td>By default, mpiP aggregates all process data at a single process
      which generates the report. &nbsp;Enabling this feature causes mpiP
      report generation to default to aggregating callsite data only for each
      individual callsite being reported. &nbsp;This dramatically reduces the
      memory requirements for large runs of applications that make many MPI
      calls. &nbsp;See run-time flags -l and -r to modify report generation behavior.</td>

    </tr>

    <tr>

      <td class="tableheader" colspan="3" rowspan="1">Stack Trace Options</td>

    </tr>

    <tr>

      <td>--enable-stackdepth=[depth]</td>

      <td>Specify maximum stacktrace depth (default is 8).</td>

      <td>Stacktraces with larger than 8 levels are sometime useful for some
      applications.</td>

    </tr>

    <tr>

      <td>--disable-libunwind</td>

      <td>Do not use libunwind to generate stack traces.</td>

      <td>Currently, libunwind seems useful on IA64-Linux and x86-Linux
      platforms, although it can conflict with the libunwind.a provided with
      the Intel compiler.</td>

    </tr>

    <tr>

      <td class="tableheader" colspan="3" rowspan="1">Address Lookup Options</td>

    </tr>

    <tr>

      <td>--enable-dwarf</td>

      <td>Use libdwarf/libelf for source lookup.</td>

      <td>libdwarf and libelf can be used for address-to-source
translation as an alternative to binutils libbfd.</td>

    </tr>

    <tr>

      <td>--disable-bfd</td>

      <td>Do not use GNU binutils libbfd for source lookup.</td>

      <td>Binutils is not always available or compatible.</td>

    </tr>

    <tr>

      <td class="tableheader" colspan="3" rowspan="1">Timing Options</td>

    </tr>

    <tr>

      <td>--with-gettimeofday</td>

      <td>Use gettimeofday for timing.</td>

      <td>Use the gettimeofday call for timing instead of the
default platform timer.</td>

    </tr>

    <tr>

      <td>--with-wtime</td>

      <td>Use MPI_Wtime for timing</td>

      <td>Use the MPI_Wtime call for timing instead of the
default platform timer.</td>

    </tr>

    <tr>

      <td>--with-clock_gettime</td>

      <td>Use clock_gettime for timing.</td>

      <td>Use the clock_gettime monotonic timer for timing instead of the default platform timer.</td>

    </tr>

    <tr>

      <td>--with-dclock</td>

      <td>Use Catamount dclock for timing.</td>

      <td>Use the dclock timer for timing on Catamount systems instead of the default platform timer.</td>

    </tr>

    <tr>

      <td>--enable-check-time</td>

      <td>Enable AIX check for negative time values.</td>

      <td>Activate IBM timing debugging code.</td>

    </tr>

    <tr>

      <td class="tableheader" colspan="3" rowspan="1">Fortran-related Options</td>

    </tr>

    <tr>

      <td>--enable-getarg</td>

      <td>Use getarg to get Fortran command line args.</td>

      <td>This is used on UNICOS to provide access to the command
line for Fortran apllications.</td>

    </tr>

    <tr>

      <td>--disable-fortran-xlate</td>

      <td>Do not translate Fortran opaque objects.</td>

      <td>Opaque object translation is not necessary on some
platforms, but necessary for Fortran applications on some 64-bit
platforms.</td>

    </tr>
    <tr>

      <td>--enable-fortranweak</td>

      <td>Generate weak symbols for additional Fortran symbol name styles.</td>

      <td>If
application objects have been created from compilers with different
Fortran symbol name styles, it may be necessary to generate weak
symbols to capture all MPI calls.</td>

    </tr>

  </tbody>
</table>

<h3>Build targets</h3>

<table border="1" cellpadding="3" cellspacing="0" width="100%">

  <tbody>

    <tr>

      <th width="25%">Command</th>

      <th>Effect</th>

    </tr>

    <tr>

      <td>make</td>

      <td>Build mpiP library or libraries for MPI profiling</td>

    </tr>

    <tr>

      <td>make install</td>

      <td>Install bin, include, lib, and slib (if applicable)
directories. The default install directory is the mpiP source
directory. The installation location can be specified with the prefix
variable as in <span class="commandline">make
prefix=[install directory] install</span>.</td>

    </tr>

    <tr>

      <td>make shared</td>

      <td>Make shared object version of library for runtime insertion (Linux).
      Support for runtime insertion on AIX and for MPI calls made within shared
      objects on Linux and AIX will be provided in a future release.</td>

    </tr>

    <tr>

      <td>make API</td>

      <td>Make standalone API library. See mpiP-API.c and mpiP-API.h for
      available features.</td>

    </tr>

    <tr>

      <td>make check</td>

      <td>Run mpiP dejagnu tests. &nbsp;Requires that runtest
is available.</td>

    </tr>

    <tr>

      <td>make add_binutils_objs</td>

      <td>For convenience, add the binutils objects to the mpiP
library. The binutils installation location must have been specified
during configuration.</td>

    </tr>

    <tr>

      <td>make add_libunwind_objs</td>

      <td>For convenience, add the libunwind objects to the mpiP
library.</td>

    </tr>

  </tbody>
</table>

<h3><a name="Linking_Examples">Example Application
Link Commands</a></h3>

<p>LLNL users can now use the srun-mpip, poe-mpip, and poe-mpip-cxx wrapper
scripts to use mpiP without re-linking their application. AIX executables would
still need to be linked with -bnoobjreorder for successful runtime address
lookup. Additionally, all LLNL installations contain the appropriate binutils
objects in the mpiP library, so the -lbfd, -liberty, and -lintl flags are not
required.  An example runtime script for mpirun is provided in the mpiP bin
directory. &nbsp;Many of the following examples use LLNL-specific compile
scripts.  </p>

<table border="1" cellpadding="3" cellspacing="0" width="100%">

  <tbody>

    <tr>

      <th>OS</th>

      <th>Compiler</th>

      <th>Language</th>

      <th>Example Link Command</th>

    </tr>

    <tr>

      <td rowspan="3" align="center">AIX</td>

      <td rowspan="3" align="center">Visual Age</td>

      <td>C</td>

      <td class="commandline">mpxlc -g -bnoobjreorder 1-hot-potato.c -o
      1-hot-potato.exe -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl
      -lm</td>

    </tr>

    <tr>

      <td>C++</td>

      <td class="commandline">mpCC_r -g -bnoobjreorder 4-demangle.C -o
      4-demangle.exe -L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty
      -lintl -lm</td>

    </tr>

    <tr>

      <td>Fortran</td>

      <td class="commandline">mpxlf -g -bnoobjreorder
sweep-ops.f -o sweep-ops.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lintl -lm</td>

    </tr>

    <tr>

      <td rowspan="9" align="center">Linux</td>

      <td rowspan="3" align="center">Intel</td>

      <td>C</td>

      <td class="commandline">mpiicc -g 1-hot-potato.c -o
1-hot-potato.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm -lmpio</td>

    </tr>

    <tr>

      <td>C++</td>

      <td class="commandline">mpiicc -g 4-demangle.C -o
4-demangle.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm -lmpio</td>

    </tr>

    <tr>

      <td>Fortran</td>

      <td class="commandline">mpiifc -g sweep-ops.f -o
sweep-ops.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm -lmpio</td>

    </tr>

    <tr>

      <td rowspan="3" align="center">PGI</td>

      <td>C</td>

      <td class="commandline">mpipgcc -g 1-hot-potato.c -o
1-hot-potato.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm -lmpio</td>

    </tr>

    <tr>

      <td>C++</td>

      <td class="commandline">mpipgCC -g 4-demangle.C -o
4-demangle.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm -lmpio</td>

    </tr>

    <tr>

      <td>Fortran</td>

      <td class="commandline">mpipgf77 -g sweep-ops.f -o
sweep-ops.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm -lmpio</td>

    </tr>

    <tr>

      <td rowspan="3" align="center">GNU</td>

      <td>C</td>

      <td class="commandline">mpicc -g 1-hot-potato.c -o
1-hot-potato.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm</td>

    </tr>

    <tr>

      <td>C++</td>

      <td class="commandline">mpiCC -g 4-demangle.C -o
4-demangle.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm</td>

    </tr>

    <tr>

      <td>Fortran</td>

      <td class="commandline">mpif77 -g sweep-ops.f -o
sweep-ops.exe -L/usr/local/tools/mpiP/lib
-lmpiP -lbfd -liberty -lm</td>

    </tr>

    <tr>

      <td>Cray X1</td>

      <td>Cray</td>

      <td>C/C++/Fortran</td>

      <td>Link with <span class="commandline">-lmpiP
-lbfd -liberty -ldwarf -lelf</span></td>

    </tr>

    <tr>

      <td>Cray XD1</td>

      <td>GNU or PGI</td>

      <td>C/C++/Fortran</td>

      <td>Link with <span class="commandline">[path_to_mpiP_install]/libmpiP.a
-lbfd
-liberty [mpich_libs]</span></td>

    </tr>

  </tbody>
</table>

<p><b>Note</b>:</p>

<ul>

  <li>If source lookup is failing during report generation, the script mpip-insert-src 
  can be used from a login node to translate addresses in the mpiP report to source information.</li>

  <li>Source lookup for callsites may fail with certain versions of binutils.
  If you are running into trouble, you may want to download a recent snapshot
  from <a href="ftp://ftp.gnu.org/gnu/binutils/">ftp://ftp.gnu.org/gnu/binutils/</a>.</li>

</ul>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="Runtime_Configuration">Run-time
Configuration of mpiP</a></h2>

<p>mpiP has several configurable parameters that a user can set via the
environment variable MPIP. Setting MPIP is done like command-line
parameters: "-t 10 -k 2". Additionally, a comma can be used to delimit
multiple parameters, as in "-t10,-k2". Currently, mpiP has several configurable
parameters.</p>

<table style="width: 90%;" border="1" cellpadding="3" cellspacing="0">

  <tbody>

    <tr>

      <th>Option</th>

      <th>Description</th>

      <th>Default</th>

    </tr>

    <tr>

      <td class="commandline">-c</td>

      <td>Generate concise version of report, omitting callsite
process-specific detail.</td>

      <td align="center">&nbsp;</td>

    </tr>

    <tr>

      <td class="commandline">-d</td>

      <td>Suppress printing of callsite detail sections.</td>

      <td align="center">&nbsp;</td>

    </tr>

    <tr>

      <td class="commandline">-e</td>

      <td>Print report data using floating-point format.</td>

      <td align="center">&nbsp;</td>

    </tr>

    <tr>

      <td class="commandline">-f dir</td>

      <td>Record output file in directory &lt;dir&gt;.</td>

      <td align="center">.</td>

    </tr>

    <tr>

      <td class="commandline">-g</td>

      <td>Enable mpiP debug mode. </td>

      <td align="center">disabled</td>

    </tr>

    <tr>

      <td class="commandline">-k n</td>

      <td>Sets callsite stack traceback depth to
&lt;n&gt;.</td>

      <td align="center">1</td>

    </tr>

    <tr>

      <td class="commandline">-l</td>

      <td>Use
less memory to generate the report by using MPI collectives to generate
callsite information on a callsite-by-callsite basis.</td>

      <td align="center">&nbsp;</td>

    </tr>

    <tr>

      <td class="commandline">-n</td>

      <td>Do not truncate full pathname of filename in callsites.</td>

      <td align="center">&nbsp;</td>

    </tr>

    <tr>

      <td class="commandline">-o</td>

      <td>Disable profiling at initialization. Application must enable
      profiling with MPI_Pcontrol().</td>

      <td align="center">&nbsp;</td>

    </tr>

    <tr>
      <td class="commandline">-p</td>
      <td style="vertical-align: top;">Point-to-point histogram reporting on message size and communicator used.<br>
      </td>
      <td style="vertical-align: top;"><br>
      </td>
    </tr>
<tr>

      <td class="commandline">-r</td>

      <td>Generate the report by aggregating data at a single task.</td>

      <td align="center">default</td>

    </tr>

    <tr>

      <td class="commandline">-s n</td>

      <td>Set hash table size to &lt;n&gt;.</td>

      <td align="center">256</td>

    </tr>

    <tr>

      <td class="commandline">-t x</td>

      <td>Set print threshold for report, where &lt;x&gt;
is the MPI percentage of time
for each callsite.</td>

      <td align="center">0.0</td>

    </tr>

    <tr>

      <td class="commandline">-v</td>

      <td>Generates both concise and verbose report output.</td>

      <td>&nbsp;</td>

    </tr>

    <tr>

      <td class="commandline">-x exe</td>

      <td>Specify the full path to the executable.</td>

      <td>&nbsp;</td>

    </tr>

    <tr>
<td class="commandline">-y</td>
<td style="vertical-align: top;">Collective histogram reporting on message size and communicator used.<br>
</td>
<td style="vertical-align: top;"><br>
</td>
    </tr>
<tr>

      <td class="commandline">-z</td>

      <td>Suppress printing of the report at MPI_Finalize.</td>

      <td>&nbsp;</td>

    </tr>

  </tbody>
</table>

<p>For example, to set the callsite stack walking depth to 2 and the report
print threshold to 10%, you simply need to define the mpiP string in your
environment, as in any of the following examples:</p>

<p class="commandline">$ export MPIP="-t 10.0 -k 2" (bash)</p>
<p class="commandline">$ export MPIP=-t10.0,-k2 (bash)</p>
<p class="commandline">$ setenv MPIP "-t 10.0 -k 2" (csh)</p>

<p> mpiP prints a message at initialization if it successfully finds this MPIP
variable.</p>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="mpiP_Output">mpiP Output</a></h2>

<p>Here is some sample output from mpiP with an application that has 4 MPI
calls.  It is broken down by sections below. Here also is the experiment setup.
<b>Note that MPIP does not capture information about ALL MPI calls</b><i>.</i>
Local calls, such as <span class="commandline">MPI_Comm_size</span>, are
omitted from the profiling library measurement to reduce perturbation and mpiP
output.</p>

<p><b>The test code</b>:</p>

<pre>  sleeptime = 10;<br>  MPI_Init (&amp;argc, &amp;argv);<br>  MPI_Comm_size (comm, &amp;nprocs);<br>  MPI_Comm_rank (comm, &amp;rank);<br>  MPI_Barrier (comm);<br>  if (rank == 0)<br>    {<br>      sleep (sleeptime);        /* slacker! delaying everyone else */<br>    }<br>  MPI_Barrier (comm);<br>  MPI_Finalize ();<br></pre>

<p><b>The code was compiled with</b>:</p>

<pre> $ mpcc -g -DAIX 9-test-mpip-time.c -o 9-test-mpip-time.exe \<br> -L.. -L/g/g2/vetter/AIX/lib -lmpiP -lbfd -liberty -lintl -lm<br></pre>

<p><b>Environment variables were set as</b>:</p>

<pre> $ export MPIP="-t 10.0"</pre>

<p><b>The example was executed on MCR like this</b>:</p>

<pre> $ srun -n 4 -ppdebug ./9-test-mpip-time.exe</pre>

<p><b>This experiment produced an output file that we can
now analyze</b>:</p>

<pre> ./9-test-mpip-time.exe.4.25972.1.mpiP</pre>

<p>Header information provides basic information about your
performance experiment.</p>

<pre>@ mpiP<br>@ Command : /g/g0/chcham/mpiP/devo/testing/./9-test-mpip-time.exe<br>@ Version : 2.8.2<br>@ MPIP Build date : Jan 10 2005, 15:15:47<br>@ Start time : 2005 01 10 16:01:32<br>@ Stop time : 2005 01 10 16:01:42<br>@ Timer Used : gettimeofday<br>@ MPIP env var : -t 10.0<br>@ Collector Rank : 0<br>@ Collector PID : 25972<br>@ Final Output Dir : .<br>@ MPI Task Assignment : 0 mcr88<br>@ MPI Task Assignment : 1 mcr88<br>@ MPI Task Assignment : 2 mcr89<br>@ MPI Task Assignment : 3 mcr89</pre>

<p>This next section provides an overview of the application's time in MPI.
Apptime is the wall-clock time from the end of <span class="commandline">MPI_Init</span> until the beginning of <span class="commandline">MPI_Finalize</span>.  MPI_Time is the wall-clock time for
all the MPI calls contained within Apptime.  MPI% shows the ratio of this
MPI_Time to Apptime. The asterisk (*) is the aggregate line for the entire
application.</p>

<pre>---------------------------------------------------------------------------<br>@--- MPI Time (seconds) ---------------------------------------------------<br>---------------------------------------------------------------------------<br>Task    AppTime    MPITime    MPI%<br>   0         10   0.000243    0.00<br>   1         10         10   99.92<br>   2         10         10   99.92<br>   3         10         10   99.92<br>   *         40         30   74.94</pre>

<p> The callsite section identifies all the MPI callsites within the
application.  The first number is the callsite ID for this mpiP file. The next
column shows the type of MPI call (w/o the MPI_ prefix). The name of the
function that contains this MPI call is next, followed by the file name and
line number.  Finally, the last column shows the PC, or program counter, for
that MPI callsite.  Note that the default setting for callsite stack walk depth
is 1. Other settings will enumerate callsites by the entire stack trace rather
than the single callsite alone.</p>

<pre>---------------------------------------------------------------------------<br>@--- Callsites: 2 ---------------------------------------------------------<br>---------------------------------------------------------------------------<br> ID Lev File/Address        Line Parent_Funct             MPI_Call<br>  1   0 9-test-mpip-time.c    52 main                     Barrier<br>  2   0 9-test-mpip-time.c    61 main                     Barrier</pre>

<p>The aggregate time section is a very quick overview of the top twenty MPI
callsites that consume the most aggregate time in your application.  Call
identifies the type of MPI function. Site provides the callsite ID (as listed
in the callsite section). Time is the aggregate time for that callsite in
milliseconds.  The next two columns show the ratio of that aggregate time to
the total application time and to the total MPI time, respectively. The COV
column indicates the variation in times of individual processes for this
callsite by presenting the coefficient of variation as calculated from the
individual process times. A larger value indicates more variation between the
process times.</p>

<pre>---------------------------------------------------------------------------<br>@--- Aggregate Time (top twenty, descending, milliseconds) ----------------<br>---------------------------------------------------------------------------<br>Call                 Site       Time    App%    MPI%     COV<br>Barrier                 2      3e+04   75.00  100.00    0.67<br>Barrier                 1      0.405    0.00    0.00    0.59</pre>

<p>The next section is similar to the aggregate time section, although it
reports on the top 20 callsites for total sent message sizes. For example:</p>

<pre>---------------------------------------------------------------------------<br>@--- Aggregate Sent Message Size (top twenty, descending, bytes) ----------<br>---------------------------------------------------------------------------<br>Call                 Site      Count      Total    Avrg      MPI%<br>Send                    7        320   1.92e+06   6e+03     99.96<br>Bcast                   1         12        336      28      0.02</pre>

<p>The final sections are the ad nauseum listing of the statistics for each
callsite across all tasks, followed by an aggregate line (indicated by an
asterisk in the Rank column). The first section is for operation time followed
by the section for message sizes.</p>

<pre>---------------------------------------------------------------------------<br>@--- Callsite Time statistics (all, milliseconds): 8 ----------------------<br>---------------------------------------------------------------------------<br>Name              Site Rank  Count      Max     Mean      Min   App%   MPI%<br>Barrier              1    0      1    0.107    0.107    0.107   0.00  44.03<br>Barrier              1    *      4    0.174    0.137    0.107   0.00   0.00<br><br>Barrier              2    0      1    0.136    0.136    0.136   0.00  55.97<br>Barrier              2    1      1    1e+04    1e+04    1e+04  99.92 100.00<br>Barrier              2    2      1    1e+04    1e+04    1e+04  99.92 100.00<br>Barrier              2    3      1    1e+04    1e+04    1e+04  99.92 100.00<br>Barrier              2    *      4    1e+04  7.5e+03    0.136  74.94 100.00</pre>

<p>Remember that we configured MPIP to not print lines where MPI% was less than
10%. All aggregate lines are printed regardless of the configuration
settings.</p>

<table border="1" cellpadding="3" cellspacing="0" width="70%">

  <tbody>

    <tr>

      <th>Column</th>

      <th>Description</th>

    </tr>

    <tr>

      <td>Name</td>

      <td>Name of the MPI function at that callsite.</td>

    </tr>

    <tr>

      <td>Site</td>

      <td>Callsite ID as listed in the callsite section above.</td>

    </tr>

    <tr>

      <td>Rank</td>

      <td>Task rank in MPI_COMM_WORLD.</td>

    </tr>

    <tr>

      <td>Count</td>

      <td>Number of times this call was executed.</td>

    </tr>

    <tr>

      <td>Max</td>

      <td>Maximum wall-clock time for one call.</td>

    </tr>

    <tr>

      <td>Mean</td>

      <td>Arithmetic mean of the wall-clock time for one call.</td>

    </tr>

    <tr>

      <td>Min</td>

      <td>Minimum wall-clock time for one call.</td>

    </tr>

    <tr>

      <td>App%</td>

      <td>Ratio of time for this call to the overall application
time for each task.</td>

    </tr>

    <tr>

      <td>MPI%</td>

      <td>Ratio of time for this call to the overall MPI time for
each task.</td>

    </tr>

  </tbody>
</table>

<p> The aggregate result for each call has the same measurement meaning;
however, the statistics are gathered across all tasks and compared with the
aggregate application and MPI times.</p>

<p> The section for sent message sizes has a similar format:</p>

<pre>---------------------------------------------------------------------------<br>@--- Callsite Message Sent statistics (all, sent bytes) -------------------<br>---------------------------------------------------------------------------<br>Name              Site Rank   Count       Max      Mean       Min       Sum<br>Send                 5    0      80      6000      6000      6000   4.8e+05<br>Send                 5    1      80      6000      6000      6000   4.8e+05<br>Send                 5    2      80      6000      6000      6000   4.8e+05<br>Send                 5    3      80      6000      6000      6000   4.8e+05<br>Send                 5    *     320      6000      6000      6000   1.92e+06</pre>

<table border="1" cellpadding="3" cellspacing="0" width="70%">

  <tbody>

    <tr>

      <th>Column</th>

      <th>Description</th>

    </tr>

    <tr>

      <td>Name</td>

      <td>Name of the MPI function at that callsite.</td>

    </tr>

    <tr>

      <td>Site</td>

      <td>Callsite ID as listed in the callsite section above.</td>

    </tr>

    <tr>

      <td>Rank</td>

      <td>Task rank in MPI_COMM_WORLD.</td>

    </tr>

    <tr>

      <td>Count</td>

      <td>Number of times this call was executed.</td>

    </tr>

    <tr>

      <td>Max</td>

      <td>Maximum sent message size in bytes for one call.</td>

    </tr>

    <tr>

      <td>Mean</td>

      <td>Arithmetic mean of the sent message sizes in bytes for
one call.</td>

    </tr>

    <tr>

      <td>Min</td>

      <td>Minimum sent message size in bytes for one call.</td>

    </tr>

    <tr>

      <td>Sum</td>

      <td>Total of all message sizes for this operation and
callsite.</td>

    </tr>

  </tbody>
</table>

<p>The format of MPI I/O report section is very similar to the
sent message sizes section:</p>

<pre>---------------------------------------------------------------------------<br>@--- Callsite I/O statistics (all, I/O bytes) -----------------------------<br>---------------------------------------------------------------------------<br>Name              Site Rank   Count       Max      Mean       Min       Sum<br>File_read            1    0      20        64        64        64      1280<br>File_read            1    1      20        64        64        64      1280<br>File_read            1    *      40        64        64        64      2560<br><br></pre>

<h3><a name="Report_Viewers">Report Viewers</a></h3>

<ul>

  <li>The <a href="http://www.llnl.gov/CASC/tool_gear/">Tool
Gear</a> project has a Qt mpiP viewer. LLNL users can run this as
mpipview.</li>

</ul>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="Controlling_Scope">Controlling the
Scope of mpiP Profiling
in your Application</a></h2>

<p>In mpiP, you can limit the scope of profiling measurements to specific
regions of your code using the <span class="commandline">MPI_Pcontrol(int
level)</span> subroutine. A value of zero disables mpiP profiling, while any
nonzero value enables profiling. To disable profiling initially at MPI_Init,
use the <span class="commandline">-o</span> configuration option. mpiP will
only record information about MPI commands encountered between activation and
deactivation. There is no limit to the number to times that an application can
activate profiling during execution.</p>

<p> For example, in your application you can capture the MPI activity for
timestep 5 only using Pcontrol. Remember to set the mpiP environment variable
to include <span class="commandline">-o</span> when using this feature.</p>

<pre>for(i=1; i &lt; 10; i++)<br>{<br>  switch(i)<br>  {<br>    case 5:<br>      MPI_Pcontrol(1);<br>      break;<br>    case 6:<br>      MPI_Pcontrol(0);<br>      break;<br>    default:<br>      break;<br>  }<br>  /* ... compute and communicate for one timestep ... */<br>}</pre>

<br>

<h3>Arbitrary Report Generation</h3>

<p>You can also generate arbitrary reports by making calls to <span class="commandline">MPI_Pcontrol()</span>with an argument of 3 or 4 (see table
below).  The first report generated will have the default report filename.
Subsequent report files will have an index number included, such as <span class="commandline">sweep3d.mpi.4.7371.1.mpiP,
sweep3d.mpi.4.7371.2.mpiP,</span>etc.  The final report will still be generated
during MPI_Finalize. <b>NOTE:</b> In the current release, callsite IDs will not
be consistent between reports. Comparison of callsite data between reports must
be done by source location and callstack.</p>

<p>MPI_Pcontrol features should be fully functional for C/C++ as
well as Fortran.</p>

<table border="1" cellpadding="3" cellspacing="0" width="40%">

  <tbody>

    <tr>

      <td align="center"><span style="font-weight: bold;">Pcontrol Argument</span></td>

      <td align="center"><span style="font-weight: bold;">Behavior</span></td>

    </tr>

    <tr>

      <td align="right">0</td>

      <td>Disable profiling.</td>

    </tr>

    <tr>

      <td align="right">1</td>

      <td>Enable Profiling.</td>

    </tr>

    <tr>

      <td align="right">2</td>

      <td>Reset all callsite data.</td>

    </tr>

    <tr>

      <td align="right">3</td>

      <td>Generate verbose report.</td>

    </tr>

    <tr>

      <td align="right">4</td>

      <td>Generate concise report.</td>

    </tr>

  </tbody>
</table>

<p>If you want to generate individual reports each time a section of code is
executed, but don't want the profile data to accumulate, you can specify code
to reset the profile data, profile, and then generate reports. &nbsp;For
example:</p>

<pre>for(i=1; i &lt; 10; i++)<br>{<br>  switch(i)<br>  {<br>    case 5:<br>      MPI_Pcontrol(2); // make sure profile data is reset<br>      MPI_Pcontrol(1); // enable profiling<br>      break;<br>    case 6:<br>      MPI_Pcontrol(3); // generate verbose report<br>      MPI_Pcontrol(4); // generate concise report<br>      MPI_Pcontrol(0); // disable profiling<br>      break;<br>    default:<br>      break;<br>  }<br>  /* ... compute and communicate for one timestep ... */<br>}</pre>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="Caveats">Caveats</a></h2>

<ul>

  <li>If mpiP has problems with the source code translation, you might be able
  to decode the program counters on LLNL systems with some of the following
  techniques.  You can use instmap, addr2line, or look at the assembler code
  itself.</li>

  <li>Compiler transformations like loop unrolling can sometimes make one
  source code line appear as many different PCs. You can verify this by looking
  at the assembler. In my experience, both instmap and addr2line do a pretty
  good job of mapping these transformed PCs into a file name and line number.
  <ul>

      <li>instmap&#8212;an IBM utility</li>

      <li>addr2line&#8212;a gnu tool</li>

      <li>look at the assembler listing, or with GNU's objdump (<span class="commandline">-d
-S</span>)</li>

      <li>use Totalview or gdb to translate the PC</li>

    </ul>

  </li>

  <li>There are known incompatibilities with certain binutils versions and
  recent versions of the IBM compilers. As of this release, a fix has not been
  incorporated into binutils, however, using the <span class="commandline">-bnoobjreorder</span> option is a valid work-around.</li>

  <li>In one case, we encountered problems on IBM machines with source lookup
  of 64-bit Fortran applications.  It appears that an incorrect compiler
  configuration file was being used, incorrectly matching debugging information
  and PC values. We addressed this by using the link flag -bpT:0x100000000.
  </li>

  <li>Issues when stack walking optimized applications:
    <ul>

      <li>Applications compiled with gcc may return incorrect parent functions;
      however, the file and line number information may be correct.</li>

      <li>Applications compiled with the Intel compiler may not be able to
      identify parent stack frames.</li>

    </ul>

  </li>

  <li>If you are calling MPI functions from within dynamically loaded objects,
  you may need to recompile the library as a shared object.</li>

  <li>We have encountered occaisional negative report values on Linux and AIX
  systems. We will continue to investigate this issue, but it is possible that
  this behavior may be experienced with mpiP.</li>

</ul>

<p class="sectionreturn"><a href="#top">Top</a></p>

<blockquote>
  <hr width="75%"></blockquote>

<h2><a name="Profiled_Routines">MPI Routines
Profiled with mpiP</a></h2>

<p>MPI_Allgather<br>

MPI_Allgatherv<br>

MPI_Allreduce<br>

MPI_Alltoall<br>

MPI_Alltoallv<br>

MPI_Attr_delete<br>

MPI_Attr_get<br>

MPI_Attr_put<br>

MPI_Barrier<br>

MPI_Bcast<br>

MPI_Bsend<br>

MPI_Bsend_init<br>

MPI_Buffer_attach<br>

MPI_Buffer_detach<br>

MPI_Cancel<br>

MPI_Cart_coords<br>

MPI_Cart_create<br>

MPI_Cart_get<br>

MPI_Cart_map<br>

MPI_Cart_rank<br>

MPI_Cart_shift<br>

MPI_Cart_sub<br>

MPI_Cartdim_get<br>

MPI_Comm_create<br>

MPI_Comm_dup<br>

MPI_Comm_group<br>

MPI_Comm_remote_group<br>

MPI_Comm_remote_size<br>

MPI_Comm_split<br>

MPI_Comm_test_inter<br>

MPI_Dims_create<br>

MPI_Error_class<br>

MPI_File_close<br>

MPI_File_open<br>

MPI_File_preallocate<br>

MPI_File_read<br>

MPI_File_read_all<br>

MPI_File_read_at<br>

MPI_File_seek<br>

MPI_File_set_view<br>

MPI_File_write<br>

MPI_File_write_all<br>

MPI_File_write_at<br>

MPI_Gather<br>

MPI_Gatherv<br>

MPI_Graph_create<br>

MPI_Graph_get<br>

MPI_Graph_map<br>

MPI_Graph_neighbors<br>

MPI_Graph_neighbors_count<br>

MPI_Graphdims_get<br>

MPI_Group_compare<br>

MPI_Group_difference<br>

MPI_Group_excl<br>

MPI_Group_free<br>

MPI_Group_incl<br>

MPI_Group_intersection<br>

MPI_Group_translate_ranks<br>

MPI_Group_union<br>

MPI_Ibsend <br>

MPI_Intercomm_create <br>

MPI_Intercomm_merge <br>

MPI_Iprobe<br>

MPI_Irecv<br>

MPI_Irsend<br>

MPI_Isend<br>

MPI_Issend<br>

MPI_Keyval_create<br>

MPI_Keyval_free<br>

MPI_Pack<br>

MPI_Probe<br>

MPI_Recv<br>

MPI_Recv_init<br>

MPI_Reduce<br>

MPI_Reduce_scatter<br>

MPI_Request_free<br>

MPI_Rsend<br>

MPI_Rsend_init<br>

MPI_Scan<br>

MPI_Scatter<br>

MPI_Scatterv<br>

MPI_Send<br>

MPI_Send_init<br>

MPI_Sendrecv<br>

MPI_Sendrecv_replace<br>

MPI_Ssend<br>

MPI_Ssend_init<br>

MPI_Start<br>

MPI_Startall<br>

MPI_Test<br>

MPI_Testall<br>

MPI_Testany<br>

MPI_Testsome<br>

MPI_Topo_test<br>

MPI_Type_commit<br>

MPI_Type_free<br>

MPI_Type_get_contents<br>

MPI_Type_get_envelope<br>

MPI_Unpack<br>

MPI_Wait<br>

MPI_Waitall<br>

MPI_Waitany<br>

MPI_Waitsome<br>

MPI_Win_complete <br>

MPI_Win_create <br>

MPI_Win_fence <br>

MPI_Win_free <br>

MPI_Win_get_group <br>

MPI_Win_lock <br>

MPI_Win_post <br>

MPI_Win_start <br>

MPI_Win_test <br>

MPI_Win_unlock <br>

MPI_Win_wait <br>
</p>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="Message_Size_Routines">MPI Routines For Which mpiP Gathers Sent
Message Size Data</a></h2>

<p>MPI_Allgather<br>

MPI_Allgatherv<br>

MPI_Allreduce<br>

MPI_Alltoall<br>

MPI_Bcast<br>

MPI_Bsend<br>

MPI_Gather<br>

MPI_Gatherv<br>

MPI_Ibsend<br>

MPI_Irsend<br>

MPI_Isend<br>

MPI_Issend<br>

MPI_Reduce<br>

MPI_Rsend<br>

MPI_Scan<br>

MPI_Scatter<br>

MPI_Send<br>

MPI_Sendrecv <br>

MPI_Sendrecv_replace <br>

MPI_Ssend </p>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="IO_Routines">MPI Routines For Which mpiP Gathers I/O Data</a></h2>

<p>MPI_File_close<br>

MPI_File_open<br>

MPI_File_preallocate<br>

MPI_File_read<br>

MPI_File_read_all<br>

MPI_File_read_at<br>

MPI_File_seek<br>

MPI_File_set_view<br>

MPI_File_write<br>

MPI_File_write_all<br>

MPI_File_write_at<br>

</p>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr width="75%">
<h2><a name="RMA_Routines">MPI Routines For Which mpiP Gathers RMA Origin Data</a></h2>

<p>

MPI_Accumulate <br>

MPI_Get <br>

MPI_Put <br>

</p>

<p class="sectionreturn"><a href="#top">Top</a></p>

<hr>
<h2><a name="Adding_Calls">How to add MPI calls to
profile</a></h2>

<p>Here is an example of how to add MPI calls to mpiP, using the
MPI_Comm_spawn call as an example:<br>

</p>

<ol>

  <li>Insert the appropriate call with appropriate arguments into
the mpi.protos.txt.in file:
    <p class="commandline">int MPI_Comm_spawn ( char
*command, char *argv[], int maxprocs, MPI_Info info, int root, MPI_Comm
comm, MPI_Comm *intercomm, int array_of_errcodes[] )</p>

  </li>

  <li>Configure mpiP or, if you have already configured mpiP, run
    <span class="commandline">./config.status</span>.
  </li>

  <li>Currently, it is necessary to add entries for MPI opaque
objects to the make-wrappers.py script. &nbsp;MPI_Comm_spawn has 3
arguments that are MPI opaque object which need to be added to
make-wrappers.py dictionaries:&nbsp;</li>

  <br>

  <ol>

    <li>Add the following entries to the opaqueInArgDict:
      <p class="commandline">("MPI_Comm_spawn",
"info"):"MPI_Info",<br>

("MPI_Comm_spawn", "comm"):"MPI_Comm",</p>

    </li>

    <li>Add the following entry to the opaqueOutArgDict:&nbsp; <p class="commandline">("MPI_Comm_spawn","intercomm"):"MPI_Comm",</p>

    </li>

  </ol>

  <li>When you build mpiP, you should see an MPI_Comm_spawn
wrapper in the generated wrappers.c file.</li>

</ol>

<p class="sectionreturn"><a href="UserGuide.html#top">Top</a></p>

<hr>
<h3><a name="License">License</a></h3>

<p>
</p>

<p>Copyright (c) 2006, The Regents of the University of California. <br>

Produced at the Lawrence Livermore National Laboratory <br>

Written by Jeffery Vetter and Christopher Chambreau. <br>

UCRL-CODE-223450. <br>

All rights reserved. <br>

<span style="">&nbsp;</span><br>

This file is part of mpiP.<span style="">&nbsp; </span>For details, see
http://mpip.sourceforge.net/. <br>

<span style="">&nbsp;</span><br>

Redistribution and use in source and binary forms, with or without<br>

modification, are permitted provided that the following conditions are<br>

met:<br>

<span style="">&nbsp;</span><br>

* Redistributions of source code must retain the above copyright<br>

notice, this list of conditions and the disclaimer below.<br>

<br>

* Redistributions in binary form must reproduce the above copyright<br>

notice, this list of conditions and the disclaimer (as noted below) in<br>

the documentation and/or other materials provided with the<br>

distribution.<br>

<br>

* Neither the name of the UC/LLNL nor the names of its contributors<br>

may be used to endorse or promote products derived from this software<br>

without specific prior written permission.<br>

<br>

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS<br>

"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT
NOT<br>

LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR<br>

A PARTICULAR PURPOSE ARE DISCLAIMED.<span style="">&nbsp;
</span>IN
NO EVENT SHALL THE REGENTS OF<br>

THE UNIVERSITY OF CALIFORNIA, THE U.S. DEPARTMENT OF ENERGY OR<br>

CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,<br>

EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,<br>

PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR<br>

PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF<br>

LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING<br>

NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS<br>

SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.<br>

<span style="">&nbsp;</span><br>

<span style="">&nbsp;</span><br>

Additional BSD Notice <br>

<span style="">&nbsp;</span><br>

1. This notice is required to be provided under our contract with the<br>

U.S. Department of Energy (DOE).<span style="">&nbsp; </span>This
work was produced at the<br>

University of California, Lawrence Livermore National Laboratory under<br>

Contract No. W-7405-ENG-48 with the DOE.<br>

<span style="">&nbsp;</span><br>

2. Neither the United States Government nor the University of<br>

California nor any of their employees, makes any warranty, express or<br>

implied, or assumes any liability or responsibility for the accuracy,<br>

completeness, or usefulness of any information, apparatus, product, or<br>

process disclosed, or represents that its use would not infringe<br>

privately-owned rights.<br>

<span style="">&nbsp;</span><br>

3.<span style="">&nbsp; </span>Also, reference
herein to any
specific commercial products,<br>

process, or services by trade name, trademark, manufacturer or<br>

otherwise does not necessarily constitute or imply its endorsement,<br>

recommendation, or favoring by the United States Government or the<br>

University of California.<span style="">&nbsp; </span>The
views and
opinions of authors expressed<br>

herein do not necessarily state or reflect those of the United States<br>

Government or the University of California, and shall not be used for<br>

advertising or product endorsement purposes.<br>

</p>

<p class="sectionreturn"><a href="UserGuide.html#top">Top</a></p>

<hr>
<p>For further information please send mail to <a href="mailto:mpip-help@lists.sourceforge.net">mpip-help@lists.sourceforge.net</a>.</p>

<p class="pageinfo"> Last modified on March 13th, 2014.<br>

UCRL-CODE-223450</p>



</body></html>
