Privacy & Legal Notice <http://www.llnl.gov/disclaimer.html>


  mpiP: Lightweight, Scalable MPI Profiling

Version 2.7
3 August 2004

Jeffrey Vetter
vetterjs@ornl.gov <mailto:vetterjs@ornl.gov>

Chris Chambreau
chcham@llnl.gov <mailto:chcham@llnl.gov>

------------------------------------------------------------------------


    Contents

    * Introduction <#Introduction>
          o Downloading <#Downloading>
          o Contributing <#Contributing>
          o New Features <#New_Features>
    * Using mpiP (summary) <#Using>
          o Supported Platforms <#Supported>
    * Building mpiP <#Building>
          o Linking Examples <#Linking_Examples>
    * Run-time Configuration <#Runtime_Configuration>
    * mpiP Output <#mpiP_Output>
          o Report Viewers <#Report_Viewers>
    * Controlling mpiP Profiling Scope <#Controlling_Scope>
    * Caveats <#Caveats>
    * List of Profiled Routines <#Profiled_Routines>
    * List of Routines With Collected Sent Message Size Information
      <#Message_Size_Routines>
    * List of I/O Routines <#IO_Routines>

------------------------------------------------------------------------


    Introduction

mpiP is a lightweight profiling library for MPI applications. Because it
only collects statistical information about MPI functions, mpiP
generates considerably less overhead and much less data than tracing
tools. All the information captured by mpiP is task-local. It only uses
communication during report generation, typically at the end of the
experiment, to merge results from all of the tasks into one output file.

We have tested mpiP on a variety of C/C++/Fortran applications from 2 to
4096 processes, including a 3584-process run on ASCI Q and a
4096-process run on ASCI White. Please send your comments, questions,
and ideas for enhancements to mpip-help@llnl.gov
<mailto:mpip-help@llnl.gov>. To receive mail regarding new mpiP
releases, please subscribe to mpip-announce@llnl.gov (send e-mail with
body "subscribe mpip-announce" to majordomo@llnl.gov
<mailto:majordomo@llnl.gov>). Please also consider subscribing to
mpip-users@llnl.gov <mailto:mpip-usersp@llnl.gov> to contribute and
receive mpiP use and status information.

To learn more about performance analysis with mpiP, see Vetter, J.S. and
M.O. McCracken, "Statistical Scalability Analysis of Communication
Operations in Distributed Applications
<http://www.llnl.gov/CASC/people/vetter/vetter_pubs.html>," Proc. ACM
SIGPLAN Symp. on Principles and Practice of Parallel Programming
(PPOPP), 2001.


      Downloading

You may download the current version of mpiP from
http://www.llnl.gov/CASC/mpip/download/


      Contributing

We are constantly improving mpiP. Bug fixes and ports to new platforms
are always welcome. Many thanks to the following contributors:

    * Michael McCracken (UCSD)
    * Curt Janssen (Sandia National Laboratories)
    * Mike Campbell (UIUC)
    * Jim Brandt (Sandia National Laboratories)


      New Features with v2.7

Release v2.7 corrects some outstanding issues and provides the following
new features:

    * MPI I/O Reporting
    * New Platform and compiler support, including x86-64-Linux,
      ia64-Linux, AIX 5.2
    * Aribtrary report generation with MPI_Pcontrol(2)
    * Greater configuration flexibility

Please note that mpiP v2.6 was only available on LLNL systems.

Top <#top>

------------------------------------------------------------------------


    Using mpiP

Using mpiP is very simple. Because it gathers MPI information through
the MPI profiling layer, mpiP is a link-time library. That is, you don't
have to recompile your application to use mpiP. Note that you might have
to recompile to include the '-g' option. This is important if you want
mpiP to decode the PC to a source code filename and line number
automatically. mpiP will work without -g, but mileage may vary.

To compile a simple program on LLNL AIX, you need to add the following
libraries to your compile/link line:

    -L${mpiP_root}/lib -lmpiP -lbfd -liberty -lintl -lm

For example, the new mpiP link line becomes

    $ mpcc -g -O mpi-foo.c -o mpi-foo.exe -L${mpiP_root}/lib -lmpiP
    -lbfd -liberty -lintl -lm

from

    $ mpcc -O mpi-foo.c -o mpi-foo.exe

Make sure the mpiP library appears before the MPI library on your link
line. The three libraries (-lbfd -liberty -lintl) provide support for
decoding the symbol information; they are part of GNU binutils.

Run your application. You can verify that mpiP is working by identifying
the header and trailer in standard out.

    0:mpiP:
    0:mpiP: mpiP V2.7 (Build Jun 22 2004/11:55:57)
    0:mpiP: Direct questions and errors to mpip-help@llnl.gov
    0:mpiP:
    0:mpiP:
    0:mpiP: found 21557 symbols in file [./9-test-mpip-time.exe]
    0:mpiP:
    0:mpiP: Storing mpiP output in [./9-test-mpip-time.exe.4.37266.mpiP].
    0:mpiP:

By default, the output file is written to the current directory of the
application. mpiP files are always much smaller than trace files, so
writing them to this directory is safe.


      Supported Platforms

The 2.7 release of mpiP supports Linux, Tru64, and AIX. Please contact
us with bug reports or questions regarding these platforms. Note that
mpiP will not work with MPICH 1.1 nor with the IBM signal-based MPI
library (libmpi.a, as opposed to the thread-based implementation
libmpi_r.a). The following table indicates platforms where mpiP was
succesfully run and any requirements for that platform.

Platform 	OS 	Compiler 	MPI 	binutils 	Requirements
IA32-Linux 	2.4.21 CHAOS Kernel 	Intel 8.0
PGI 5.1-6 	Quadrics MPI 1.24-33 	2.15 	Some binutils mods required for
Intel 8.0 support. Please send mail to the help list for more information.
2.4.22 Kernel 	gcc 3.3.1 	mpich-1.2.5.2
IBM Power3/4 	AIX 5.1
AIX 5.2 	VA C 6.0/
VA Fortran 8.1 	PE 3.2
PE 4.1 	2.15 	Binutils currently requires that applications are linked
with the -bnoobjreorder linker flag. Other mods are required for 64-bit
builds of binutils.
Alpha EV67 	Tru64 5.1b 	Compaq C 6.4/
Fortran 5.5 	Quadrics RMS 2.5 2.14 	2.15 	 

Top <#top>

------------------------------------------------------------------------


    Building mpiP

Currently, mpiP requires a compatible GNU binutils
<http://sources.redhat.com/binutils> installation. The binutils include
and lib directories must be specified with the --with-include and
--with-ldflags configure flags. It is very likely that the compilers
will need to be indentified as well, with the --with-cc, --with-cxx, and
--with-f77 flags. After running configure with the appropriate
arguments, "make" will build the appropriate libraries in the mpiP
directory.

There are many new configuration options available. They are as follows:

Flag 	Effect 	Description
--enable-demangling	Create libmpiPdmg.a	Demangling support is
implemented in the library libmpiPdmg.a. This library will be built if
--enable-demangling=[GNU|IBM|Compaq] is provided to the configure
script. Use GNU for the Intel compiler.
--enable-stackdepth	Specify maximum stacktrace depth (default is 8).
Stacktraces with larger than 8 levels are sometime useful for some
applications.
--disable-bfd	Do not use GNU binutils libbfd for source lookup.	Binutils
is not always available or compatible.
--disable-libunwind	Do not use libunwind to generate stack traces.
Currently, libunwind seems useful on IA64-Linux and x86-Linux platforms.
--disable-mpi-io	Disable MPI-I/O reporting.	Useful for generating an
mpiP library without MPI I/O for MPI implementations such as Quadrics
MPI that has a separate MPI I/O library.
--enable-getarg	Use getarg to get fortran command line args.	This is
required for the Intel 8.0 compiler in order to identify the name of
Fortran applications.
--enable-check-time	Enable AIX check for negative time values.	Activate
IBM timing debugging code.


      Example Application Link Commands for LLNL Machines

OS 	Compiler 	Language 	Example Link Command
AIX 	Visual Age 	C 	mpxlc -g -bnoobjreorder 1-hot-potato.c -o
1-hot-potato.exe -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty
-lintl -lm
C++ 	mpCC_r -g -bnoobjreorder 4-demangle.C -o 4-demangle.exe
-L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl -lm
Fortran 	mpxlf -g -bnoobjreorder sweep-ops.f -o sweep-ops.exe
-L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm
Linux 	Intel 	C 	mpiicc -g 1-hot-potato.c -o 1-hot-potato.exe
-L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm -lmpio
C++ 	mpiicc -g 4-demangle.C -o 4-demangle.exe
-L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl -lm -lmpio
Fortran 	mpiifc -g sweep-ops.f -o sweep-ops.exe
-L/usr/local/tools/mpiP/lib -lmpiPifc -lbfd -liberty -lintl -lm -lmpio
PGI 	C 	mpipgcc -g 1-hot-potato.c -o 1-hot-potato.exe
-L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm -lmpio
C++ 	mpipgCC -g 4-demangle.C -o 4-demangle.exe
-L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl -lm -lmpio
Fortran 	mpipgf77 -g sweep-ops.f -o sweep-ops.exe
-L/usr/local/tools/mpiP/lib -lmpiPpgf -lbfd -liberty -lintl -lm -lmpio
GNU 	C 	mpicc -g 1-hot-potato.c -o 1-hot-potato.exe
-L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm
C++ 	mpiCC -g 4-demangle.C -o 4-demangle.exe -L/usr/local/tools/mpiP/lib
-lmpiPdmg -lbfd -liberty -lintl -lm
Fortran 	mpif77 -g sweep-ops.f -o sweep-ops.exe
-L/usr/local/tools/mpiP/lib -lmpiPg77 -lbfd -liberty -lintl -lm
Tru64 	Compaq 	C 	cc -g -I/usr/lib/mpi/include 1-hot-potato.c -o
1-hot-potato.exe -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty
-lintl -lm -lmpi -lpmpi -lexc
C++ 	cxx -g 4-demangle.C -o 4-demangle.exe -L/usr/local/tools/mpiP/lib
-lmpiPdmg -lbfd -liberty -lintl -lm -lmpi -lpmpi -lexc -lmld
Fortran 	f77 -g -I/usr/lib/mpi/include sweep-ops.f -o sweep-ops.exe
-L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm -lmpi
-lpmpi -lexc

*Note*:

    * On Linux, the mpiP Fortran libraries have unique names:
      libmpiPifc.a for the Intel compiler, libmpiPpgf.a for the PGI
      compiler, and libmpiPg77.a for the GNU compiler.
    * On Tru64, the exception library flag (-lexc) must be added to the
      link command.
    * Source lookup for callsites may fail with certain versions of
      binutils. If you are running into trouble, you may want to
      download a recent snapshot from
      ftp://sources.redhat.com/pub/binutils/snapshots.

Top <#top>

------------------------------------------------------------------------


    Run-time Configuration of mpiP

mpiP has several configurable parameters that a user can set via the
environment variable MPIP. The setting for MPIP looks like command-line
parameters: "-t 10 -k 2". Currently, mpiP has several configurable
parameters.

Option 	Description 	Default
-c 	Add COV calculation field to report for indicating process
distribution. 	disabled
-e 	Print report data using floating-point format. 	 
-f dir 	Record output file in directory <dir>. 	.
-g 	Enable mpiP debug mode. 	disabled
-k n 	Sets callsite stack traceback depth to <n>. 	1
-n 	Do not truncate full pathname of filename in callsites. 	 
-o 	Disable profiling at initialization. Application must enable
profiling with MPI_Pcontrol(). 	 
-s n 	Set hash table size to <n>. 	256
-t x 	Set print threshold for report, where <x> is the MPI percentage of
time for each callsite. 	0.0

For example, to set the callsite stack walking depth to 2 and the report
print threshold to 10%, you simply need to define the mpiP string in
your environment:

$ export MPIP="-t 10.0 -k 2"

mpiP prints a message at initialization if it successfully finds this
MPIP variable.

Top <#top>

------------------------------------------------------------------------


    mpiP Output

Here is some sample output from mpiP with an application that has 4 MPI
calls. It is broken down by sections below. Here also is the experiment
setup. *Note that MPIP does not capture information about ALL MPI
calls*/./ Local calls, such as MPI_Comm_size, are omitted from the
profiling library measurement to reduce perturbation and mpiP output.

*The test code*:

  sleeptime = 10;
  MPI_Init (&argc, &argv);
  MPI_Comm_size (comm, &nprocs);
  MPI_Comm_rank (comm, &rank);
  MPI_Barrier (comm);
  if (rank == 0)
    {
      sleep (sleeptime);        /* slacker! delaying everyone else */
    }
  MPI_Barrier (comm);
  MPI_Finalize ();

*The code was compiled with*:

  $ mpcc -g -g -DAIX 9-test-mpip-time.c -o 9-test-mpip-time.exe  \
       -L.. -L/g/g2/vetter/AIX/lib  -lmpiP -lbfd -liberty -lintl -lm

*Environment variables were set as*:

  $ export MPIP="-t 10.0"

*The example was executed on Snow like this*:

  $ ./9-test-mpip-time.exe -procs 4 -nodes 1

*This experiment produced an output file that we can now analyze*:

  ./9-test-mpip-time.exe.4.37266.mpiP

Header information provides basic information about your performance
experiment.

@ mpiP
@ Command : ./9-test-mpip-time.exe
@ Version                  : 2.5
@ MPIP Build date          : Aug 28 2001, 11:55:57
@ Start time               : 2001 08 28 12:07:18
@ Stop time                : 2001 08 28 12:07:28
@ MPIP env var : -t 10.0

@ Collector Rank           : 0
@ Collector PID            : 37266
@ Final Output Dir         : .
@ MPI Task Assignment      : 0 snow06.llnl.gov
@ MPI Task Assignment      : 1 snow06.llnl.gov
@ MPI Task Assignment      : 2 snow06.llnl.gov
@ MPI Task Assignment      : 3 snow06.llnl.gov

This next section provides an overview of the application's time in MPI.
Apptime is the wall-clock time from the end of MPI_Init until the
beginning of MPI_Finalize. MPI_Time is the wall-clock time for all the
MPI calls contained within Apptime. MPI% shows the ratio of this
MPI_Time to Apptime. The asterisk (*) is the aggregate line for the
entire application.

---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime    MPI%
   0         10   0.000243    0.00
   1         10         10   99.92
   2         10         10   99.92
   3         10         10   99.92
   *         40         30   74.94

The callsite section identifies all the MPI callsites within the
application. The first number is the callsite ID for this mpiP file. The
next column shows the type of MPI call (w/o the MPI_ prefix). The name
of the function that contains this MPI call is next, followed by the
file name and line number. Finally, the last column shows the PC, or
program counter, for that MPI callsite. Note that the default setting
for callsite stack walk depth is 1. Other settings will enumerate
callsites by the entire stack trace rather than the single callsite alone.

---------------------------------------------------------------------------
@--- Callsites: 2 ---------------------------------------------------------
---------------------------------------------------------------------------
 ID Lev File            Line Parent_Funct         MPI_Call
  1   0 9-test-mpip-time.c   47 .main                          Barrier
  2   0 9-test-mpip-time.c   56 .main                          Barrier

The aggregate time section is a very quick overview of the top twenty
MPI callsites that consume the most aggregate time in your application.
Call identifies the type of MPI function. Site provides the callsite ID
(as listed in the callsite section). Time is the aggregate time for that
callsite in milliseconds. The final two columns show the ratio of that
aggregate time to the total application time and to the total MPI time,
respectively.

---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site      Time     App%     MPI%
Barrier                 2      3e+04   74.94   100.00
Barrier                 1      0.547    0.00     0.00

The next section is similar to the aggregate time section, although it
reports on the top 20 callsites for total sent message sizes. For example:

--------------------------------------------------------------------------- 
@--- Aggregate Sent Message Size (top twenty, descending, bytes) ---------- 
--------------------------------------------------------------------------- 
Call                 Site      Count      Total    Avrg      MPI% 
Send                    7        320   1.92e+06   6e+03     99.96 
Bcast                   1         12        336      28      0.02

The final sections are the ad nauseum listing of the statistics for each
callsite across all tasks, followed by an aggregate line (indicated by
an asterisk in the Rank column). The first section is for operation time
followed by the section for message sizes.

---------------------------------------------------------------------------
@--- Callsite Time statistics (all, milliseconds): 8 ----------------------
---------------------------------------------------------------------------
Name              Site Rank  Count      Max     Mean      Min   App%   MPI%
Barrier              1    0      1    0.107    0.107    0.107   0.00  44.03
Barrier              1    *      4    0.174    0.137    0.107   0.00   0.00

Barrier              2    0      1    0.136    0.136    0.136   0.00  55.97
Barrier              2    1      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    2      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    3      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    *      4    1e+04  7.5e+03    0.136  74.94 100.00

Remember that we configured MPIP to not print lines where MPI% was less
than 10%. All aggregate lines are printed regardless of the
configuration settings.

Column 	Description
Name 	Name of the MPI function at that callsite.
Site 	Callsite ID as listed in the callsite section above.
Rank 	Task rank in MPI_COMM_WORLD.
Count 	Number of times this call was executed.
Max 	Maximum wall-clock time for one call.
Mean 	Arithmetic mean of the wall-clock time for one call.
Min 	Minimum wall-clock time for one call.
App% 	Ratio of time for this call to the overall application time for
each task.
MPI% 	Ratio of time for this call to the overall MPI time for each task.

The aggregate result for each call has the same measurement meaning;
however, the statistics are gathered across all tasks and compared with
the aggregate application and MPI times.

The section for sent message sizes has a similar format:

---------------------------------------------------------------------------
@--- Callsite Message Sent statistics (all, sent bytes) -------------------
---------------------------------------------------------------------------
Name              Site Rank   Count       Max      Mean       Min       Sum
Send                 5    0      80      6000      6000      6000   4.8e+05
Send                 5    1      80      6000      6000      6000   4.8e+05
Send                 5    2      80      6000      6000      6000   4.8e+05
Send                 5    3      80      6000      6000      6000   4.8e+05
Send                 5    *     320      6000      6000      6000   1.92e+06

Column 	Description
Name 	Name of the MPI function at that callsite.
Site 	Callsite ID as listed in the callsite section above.
Rank 	Task rank in MPI_COMM_WORLD.
Count 	Number of times this call was executed.
Max 	Maximum sent message size in bytes for one call.
Mean 	Arithmetic mean of the sent message sizes in bytes for one call.
Min 	Minimum sent message size in bytes for one call.
Sum 	Total of all message sizes for this operation and callsite.

The format of MPI I/O report section is very similar to the sent message
sizes section:

---------------------------------------------------------------------------
@--- Callsite I/O statistics (all, I/O bytes) -----------------------------
---------------------------------------------------------------------------
Name              Site Rank   Count       Max      Mean       Min       Sum
File_read            1    0      20        64        64        64      1280
File_read            1    1      20        64        64        64      1280
File_read            1    *      40        64        64        64      2560


      Report Viewers

Several projects are developing tools for viewing and analyzing mpiP
report data:

    * The Tool Gear <http://www.llnl.gov/CASC/tool_gear/> project has a
      Qt mpiP viewer.
    * The TAU
      <http://www.cs.uoregon.edu/research/paracomp/tau/tautools/>
      project will support mpiP report viewing and database upload of
      mpiP data from within the ParaProf tool as of a 8/6/04 release
      with documentation updates to follow.

Top <#top>

------------------------------------------------------------------------


    Controlling the Scope of mpiP Profiling in your Application

In mpiP, you can limit the scope of profiling measurements to specific
regions of your code using the MPI_Pcontrol(int level) subroutine. A
value of zero disables mpiP profiling, while any nonzero value enables
profiling. To disable profiling initially at MPI_Init, use the -o
configuration option. mpiP will only record information about MPI
commands encountered between activation and deactivation. There is no
limit to the number to times that an application can activate profiling
during execution.

For example, in your application you can capture the MPI activity for
timestep 5 only using Pcontrol. Remember to set the mpiP environment
variable to include -o when using this feature.

for(i=1; i < 10; i++)
{
  switch(i)
  {
    case 5:
      MPI_Pcontrol(1);
      break;
    case 6:
      MPI_Pcontrol(0);
      break;
    default:
      break;
  }
  /* ... compute and communicate for one timestep ... */
}

You can also generate arbitrary reports by making calls to
MPI_Pcontrol(2). The first report generated will have the default report
filename. Subsequent report files will have an index number included,
such as sweep3d.mpi.4.7371.1.mpiP, sweep3d.mpi.4.7371.2.mpiP,etc. The
final report will still be generated during MPI_Finalize.

MPI_Pcontrol features should be fully functional for C/C++ as well as
Fortran.

Top <#top>

------------------------------------------------------------------------


    Caveats

    * If mpiP has problems with the source code translation, you might
      be able to decode the program counters on LLNL systems with some
      of the following techniques. You can use instmap, addr2line, or
      look at the assembler code itself.
    * Compiler transformations like loop unrolling can sometimes make
      one source code line appear as many different PCs. You can verify
      this by looking at the assembler. In my experience, both instmap
      and addr2line do a pretty good job of mapping these transformed
      PCs into a file name and line number.
          o instmap?an IBM utility
          o addr2line?a gnu tool
          o look at the assembler listing, or with GNU's objdump (-d -S)
          o use Totalview or gdb to translate the PC
    * There are known incompatibilities with certain binutils versions
      and recent versions of the IBM compilers. As of this release, a
      fix has not been incorporated into binutils, however, using the
      -bnoobjreorder option is a valid work-around.
    * In one case, we encountered problems on IBM machines with source
      lookup of 64-bit Fortran applications. It appears that an
      incorrect compiler configuration file was being used, incorrectly
      matching debugging information and PC values. We addressed this by
      using the link flag -bpT:0x100000000.
    * Issues when stack walking optimized applications:
          o Applications compiled with gcc may return incorrect parent
            functions; however, the file and line number information may
            be correct.
          o Applications compiled with the Intel compiler may not be
            able to identify parent stack frames.
    * If you are calling MPI functions from within dynamically loaded
      objects, you may need to recompile the library as a shared object.
    * The mpiP library will not link with the signal-based IBM mpi library.

Top <#top>

    ------------------------------------------------------------------------


    MPI Routines Profiled with mpiP

MPI_Allgather
MPI_Allgatherv
MPI_Allreduce
MPI_Alltoall
MPI_Alltoallv
MPI_Attr_delete
MPI_Attr_get
MPI_Attr_put
MPI_Barrier
MPI_Bcast
MPI_Bsend
MPI_Bsend_init
MPI_Buffer_attach
MPI_Buffer_detach
MPI_Cancel
MPI_Cart_coords
MPI_Cart_create
MPI_Cart_get
MPI_Cart_map
MPI_Cart_rank
MPI_Cart_shift
MPI_Cart_sub
MPI_Cartdim_get
MPI_Comm_create
MPI_Comm_dup
MPI_Comm_group
MPI_Comm_remote_group
MPI_Comm_remote_size
MPI_Comm_split
MPI_Comm_test_inter
MPI_Dims_create
MPI_Error_class
MPI_File_close
MPI_File_open
MPI_File_preallocate
MPI_File_read
MPI_File_read_all
MPI_File_read_at
MPI_File_seek
MPI_File_set_view
MPI_File_write
MPI_File_write_all
MPI_File_write_at
MPI_Gather
MPI_Gatherv
MPI_Graph_create
MPI_Graph_get
MPI_Graph_map
MPI_Graph_neighbors
MPI_Graph_neighbors_count
MPI_Graphdims_get
MPI_Group_compare
MPI_Group_difference
MPI_Group_excl
MPI_Group_free
MPI_Group_incl
MPI_Group_intersection
MPI_Group_translate_ranks
MPI_Group_union
MPI_Ibsend
MPI_Intercomm_create
MPI_Intercomm_merge
MPI_Iprobe
MPI_Irecv
MPI_Irsend
MPI_Isend
MPI_Issend
MPI_Keyval_create
MPI_Keyval_free
MPI_Pack
MPI_Probe
MPI_Recv
MPI_Recv_init
MPI_Reduce
MPI_Reduce_scatter
MPI_Request_free
MPI_Rsend
MPI_Rsend_init
MPI_Scan
MPI_Scatter
MPI_Scatterv
MPI_Send
MPI_Send_init
MPI_Sendrecv
MPI_Sendrecv_replace
MPI_Ssend
MPI_Ssend_init
MPI_Start
MPI_Startall
MPI_Test
MPI_Testall
MPI_Testany
MPI_Testsome
MPI_Topo_test
MPI_Type_commit
MPI_Type_free
MPI_Type_get_contents
MPI_Type_get_envelope
MPI_Unpack
MPI_Wait
MPI_Waitall
MPI_Waitany
MPI_Waitsome

Top <#top>

------------------------------------------------------------------------


    MPI Routines For Which mpiP Gathers Sent Message Size Data

MPI_Allgather
MPI_Allgatherv
MPI_Allreduce
MPI_Alltoall
MPI_Bcast
MPI_Bsend
MPI_Gather
MPI_Gatherv
MPI_Ibsend
MPI_Irsend
MPI_Isend
MPI_Issend
MPI_Reduce
MPI_Rsend
MPI_Scan
MPI_Scatter
MPI_Send
MPI_Sendrecv
MPI_Sendrecv_replace
MPI_Ssend

Top <#top>

------------------------------------------------------------------------


    MPI Routines For Which mpiP Gathers I/O Data

MPI_File_close
MPI_File_open
MPI_File_preallocate
MPI_File_read
MPI_File_read_all
MPI_File_read_at
MPI_File_seek
MPI_File_set_view
MPI_File_write
MPI_File_write_all
MPI_File_write_at

Top <#top>

------------------------------------------------------------------------

For further information please send mail to mpip-help@llnl.gov
<mailto:mpip-help@llnl.gov>.

Last modified on August 3rd, 2004.
UCRL-CODE-2002-020, Version 2.

