   [1]Privacy & Legal Notice

                 mpiP: Lightweight, Scalable MPI Profiling

                                Version 2.7
                               3 August 2004
                               Jeffrey Vetter
                            [2]vetterjs@ornl.gov
                              Chris Chambreau
                             [3]chcham@llnl.gov
   ______________________________________________________________________

Contents

     * [4]Introduction
          + [5]Downloading
          + [6]Contributing
          + [7]New Features
     * [8]Using mpiP (summary)
          + [9]Supported Platforms
     * [10]Building mpiP
          + [11]Linking Examples
     * [12]Run-time Configuration
     * [13]mpiP Output

     * [14]Report Viewers

     [15]Controlling mpiP Profiling Scope

     [16]Caveats

     [17]List of Profiled Routines

     [18]List of Routines With Collected Sent Message Size Information

     [19]List of I/O Routines
            ____________________________________________________

Introduction

   mpiP  is a lightweight profiling library for MPI applications. Because
   it  only  collects  statistical  information about MPI functions, mpiP
   generates  considerably  less overhead and much less data than tracing
   tools.  All  the  information  captured by mpiP is task-local. It only
   uses  communication  during report generation, typically at the end of
   the experiment, to merge results from all of the tasks into one output
   file.

   We  have tested mpiP on a variety of C/C++/Fortran applications from 2
   to  4096  processes,  including  a  3584-process  run  on ASCI Q and a
   4096-process  run on ASCI White. Please send your comments, questions,
   and  ideas for enhancements to [20]mpip-help@llnl.gov. To receive mail
   regarding     new     mpiP     releases,     please    subscribe    to
   mpip-announce@llnl.gov    (send    e-mail    with    body   "subscribe
   mpip-announce"   to   [21]majordomo@llnl.gov).  Please  also  consider
   subscribing  to [22]mpip-users@llnl.gov to contribute and receive mpiP
   use and status information.

   To  learn  more about performance analysis with mpiP, see Vetter, J.S.
   and   M.O.   McCracken,   "[23]Statistical   Scalability  Analysis  of
   Communication  Operations  in  Distributed  Applications,"  Proc.  ACM
   SIGPLAN  Symp.  on  Principles  and  Practice  of Parallel Programming
   (PPOPP), 2001.

  Downloading

   You    may    download    the    current    version   of   mpiP   from
   [24]http://www.llnl.gov/CASC/mpip/download/

  Contributing

   We are constantly improving mpiP. Bug fixes and ports to new platforms
   are always welcome. Many thanks to the following contributors:
     * Michael McCracken (UCSD)
     * Curt Janssen (Sandia National Laboratories)
     * Mike Campbell (UIUC)
     * Jim Brandt (Sandia National Laboratories)

  New Features with v2.7

   Release  v2.7  corrects  some  outstanding  issues  and  provides  the
   following new features:
     * MPI I/O Reporting
     * New   Platform   and  compiler  support,  including  x86-64-Linux,
       ia64-Linux, AIX 5.2
     * Aribtrary report generation with MPI_Pcontrol(2)
     * Greater configuration flexibility

   Please note that mpiP v2.6 was only available on LLNL systems.

   [25]Top
            ____________________________________________________

Using mpiP

   Using  mpiP is very simple. Because it gathers MPI information through
   the  MPI  profiling  layer,  mpiP is a link-time library. That is, you
   don't  have  to  recompile your application to use mpiP. Note that you
   might  have to recompile to include the '-g' option. This is important
   if  you  want mpiP to decode the PC to a source code filename and line
   number automatically. mpiP will work without -g, but mileage may vary.

   To compile a simple program on LLNL AIX, you need to add the following
   libraries to your compile/link line:

     -L${mpiP_root}/lib -lmpiP -lbfd -liberty -lintl -lm

   For example, the new mpiP link line becomes

     $  mpcc  -g  -O  mpi-foo.c -o mpi-foo.exe -L${mpiP_root}/lib -lmpiP
     -lbfd -liberty -lintl -lm

   from

     $ mpcc -O mpi-foo.c -o mpi-foo.exe

   Make sure the mpiP library appears before the MPI library on your link
   line.  The three libraries (-lbfd -liberty -lintl) provide support for
   decoding the symbol information; they are part of GNU binutils.

   Run  your  application.  You  can  verify  that  mpiP  is  working  by
   identifying the header and trailer in standard out.

     0:mpiP:
     0:mpiP: mpiP V2.7 (Build Jun 22 2004/11:55:57)
     0:mpiP: Direct questions and errors to mpip-help@llnl.gov
     0:mpiP:
     0:mpiP:
     0:mpiP: found 21557 symbols in file [./9-test-mpip-time.exe]
     0:mpiP:
     0:mpiP: Storing mpiP output in
     [./9-test-mpip-time.exe.4.37266.mpiP].
     0:mpiP:

   By default, the output file is written to the current directory of the
   application.  mpiP  files are always much smaller than trace files, so
   writing them to this directory is safe.

  Supported Platforms

   The 2.7 release of mpiP supports Linux, Tru64, and AIX. Please contact
   us  with bug reports or questions regarding these platforms. Note that
   mpiP  will  not  work with MPICH 1.1 nor with the IBM signal-based MPI
   library  (libmpi.a,  as  opposed  to  the  thread-based implementation
   libmpi_r.a).  The  following  table indicates platforms where mpiP was
   succesfully run and any requirements for that platform.

   Platform OS Compiler MPI binutils Requirements
   IA32-Linux 2.4.21 CHAOS Kernel Intel 8.0
   PGI  5.1-6  Quadrics  MPI 1.24-33 2.15 Some binutils mods required for
   Intel  8.0  support.  Please  send  mail  to  the  help  list for more
   information.
   2.4.22 Kernel gcc 3.3.1 mpich-1.2.5.2
   IBM Power3/4 AIX 5.1
   AIX 5.2 VA C 6.0/
   VA Fortran 8.1 PE 3.2
   PE  4.1  2.15 Binutils currently requires that applications are linked
   with  the  -bnoobjreorder  linker  flag.  Other  mods are required for
   64-bit builds of binutils.
   Alpha EV67 Tru64 5.1b Compaq C 6.4/
   Fortran 5.5 Quadrics RMS 2.5 2.14 2.15

   [26]Top
            ____________________________________________________

Building mpiP

   Currently,  mpiP  requires a compatible GNU [27]binutils installation.
   The  binutils  include  and lib directories must be specified with the
   --with-include  and  --with-ldflags configure flags. It is very likely
   that  the  compilers  will  need  to  be indentified as well, with the
   --with-cc,  --with-cxx,  and --with-f77 flags. After running configure
   with  the  appropriate  arguments,  "make"  will build the appropriate
   libraries in the mpiP directory.

   There  are  many  new  configuration  options  available.  They are as
   follows:

   Flag Effect Description
   --enable-demangling Create libmpiPdmg.a Demangling support is
   implemented in the library libmpiPdmg.a. This library will be built if
   --enable-demangling=[GNU|IBM|Compaq] is provided to the configure
   script. Use GNU for the Intel compiler.
   --enable-stackdepth  Specify  maximum stacktrace depth (default is 8).
   Stacktraces  with  larger  than  8 levels are sometime useful for some
   applications.
   --disable-bfd  Do  not  use  GNU  binutils  libbfd  for source lookup.
   Binutils is not always available or compatible.
   --disable-libunwind  Do  not  use  libunwind to generate stack traces.
   Currently,   libunwind   seems  useful  on  IA64-Linux  and  x86-Linux
   platforms.
   --disable-mpi-io  Disable  MPI-I/O reporting. Useful for generating an
   mpiP  library without MPI I/O for MPI implementations such as Quadrics
   MPI that has a separate MPI I/O library.
   --enable-getarg  Use  getarg to get fortran command line args. This is
   required  for  the Intel 8.0 compiler in order to identify the name of
   Fortran applications.
   --enable-check-time   Enable  AIX  check  for  negative  time  values.
   Activate IBM timing debugging code.

  Example Application Link Commands for LLNL Machines

   OS Compiler Language Example Link Command
   AIX Visual Age C mpxlc -g -bnoobjreorder 1-hot-potato.c -o
   1-hot-potato.exe -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty
   -lintl -lm
   C++   mpCC_r   -g   -bnoobjreorder   4-demangle.C   -o  4-demangle.exe
   -L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl -lm
   Fortran   mpxlf   -g   -bnoobjreorder   sweep-ops.f  -o  sweep-ops.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm
   Linux   Intel   C   mpiicc   -g   1-hot-potato.c  -o  1-hot-potato.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm -lmpio
   C++ mpiicc -g 4-demangle.C -o 4-demangle.exe
   -L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl -lm -lmpio
   Fortran      mpiifc      -g      sweep-ops.f      -o     sweep-ops.exe
   -L/usr/local/tools/mpiP/lib -lmpiPifc -lbfd -liberty -lintl -lm -lmpio
   PGI     C    mpipgcc    -g    1-hot-potato.c    -o    1-hot-potato.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm -lmpio
   C++      mpipgCC      -g      4-demangle.C      -o      4-demangle.exe
   -L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl -lm -lmpio
   Fortran      mpipgf77      -g     sweep-ops.f     -o     sweep-ops.exe
   -L/usr/local/tools/mpiP/lib -lmpiPpgf -lbfd -liberty -lintl -lm -lmpio
   GNU     C     mpicc     -g    1-hot-potato.c    -o    1-hot-potato.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm
   C++ mpiCC -g 4-demangle.C -o 4-demangle.exe
   -L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl -lm
   Fortran      mpif77      -g      sweep-ops.f      -o     sweep-ops.exe
   -L/usr/local/tools/mpiP/lib -lmpiPg77 -lbfd -liberty -lintl -lm
   Tru64   Compaq   C  cc  -g  -I/usr/lib/mpi/include  1-hot-potato.c  -o
   1-hot-potato.exe  -L/usr/local/tools/mpiP/lib  -lmpiP  -lbfd  -liberty
   -lintl -lm -lmpi -lpmpi -lexc
   C++  cxx -g 4-demangle.C -o 4-demangle.exe -L/usr/local/tools/mpiP/lib
   -lmpiPdmg -lbfd -liberty -lintl -lm -lmpi -lpmpi -lexc -lmld
   Fortran  f77  -g  -I/usr/lib/mpi/include  sweep-ops.f -o sweep-ops.exe
   -L/usr/local/tools/mpiP/lib  -lmpiP  -lbfd  -liberty  -lintl -lm -lmpi
   -lpmpi -lexc

   Note:
     * On   Linux,   the   mpiP  Fortran  libraries  have  unique  names:
       libmpiPifc.a  for  the  Intel  compiler,  libmpiPpgf.a for the PGI
       compiler, and libmpiPg77.a for the GNU compiler.
     * On  Tru64, the exception library flag (-lexc) must be added to the
       link command.
     * Source  lookup  for  callsites  may  fail with certain versions of
       binutils.  If  you  are  running  into  trouble,  you  may want to
       download a recent snapshot from
       [28]ftp://sources.redhat.com/pub/binutils/snapshots.

   [29]Top
            ____________________________________________________

Run-time Configuration of mpiP

   mpiP  has  several configurable parameters that a user can set via the
   environment   variable   MPIP.   The   setting  for  MPIP  looks  like
   command-line  parameters:  "-t  10  -k 2". Currently, mpiP has several
   configurable parameters.

 Option                           Description                          Default
 -c     Add COV calculation field to report for indicating process
        distribution.                                                  disabled
 -e     Print report data using floating-point format.                   
 -f dir Record output file in directory <dir>.                            .
 -g     Enable mpiP debug mode.                                        disabled
 -k n   Sets callsite stack traceback depth to <n>.                       1
 -n     Do not truncate full pathname of filename in callsites.          
 -o     Disable profiling at initialization. Application must enable
        profiling with MPI_Pcontrol().                                   
 -s n   Set hash table size to <n>.                                      256
 -t x   Set print threshold for report, where <x> is the MPI percentage
        of time for each callsite.                                       0.0

   For  example,  to  set  the  callsite stack walking depth to 2 and the
   report  print  threshold  to  10%,  you simply need to define the mpiP
   string in your environment:

   $ export MPIP="-t 10.0 -k 2"

   mpiP  prints a message at initialization if it successfully finds this
   MPIP variable.

   [30]Top
            ____________________________________________________

mpiP Output

   Here  is  some  sample output from mpiP with an application that has 4
   MPI  calls.  It  is  broken  down  by sections below. Here also is the
   experiment  setup.  Note  that MPIP does not capture information about
   ALL  MPI  calls.  Local calls, such as MPI_Comm_size, are omitted from
   the  profiling  library  measurement  to  reduce perturbation and mpiP
   output.

   The test code:
  sleeptime = 10;
  MPI_Init (&argc, &argv);
  MPI_Comm_size (comm, &nprocs);
  MPI_Comm_rank (comm, &rank);
  MPI_Barrier (comm);
  if (rank == 0)
    {
      sleep (sleeptime);        /* slacker! delaying everyone else */
    }
  MPI_Barrier (comm);
  MPI_Finalize ();

   The code was compiled with:
  $ mpcc -g -g -DAIX 9-test-mpip-time.c -o 9-test-mpip-time.exe  \
       -L.. -L/g/g2/vetter/AIX/lib  -lmpiP -lbfd -liberty -lintl -lm

   Environment variables were set as:
  $ export MPIP="-t 10.0"

   The example was executed on Snow like this:
  $ ./9-test-mpip-time.exe -procs 4 -nodes 1

   This experiment produced an output file that we can now analyze:
  ./9-test-mpip-time.exe.4.37266.mpiP

   Header  information  provides basic information about your performance
   experiment.
@ mpiP
@ Command : ./9-test-mpip-time.exe
@ Version                  : 2.5
@ MPIP Build date          : Aug 28 2001, 11:55:57
@ Start time               : 2001 08 28 12:07:18
@ Stop time                : 2001 08 28 12:07:28
@ MPIP env var : -t 10.0

@ Collector Rank           : 0
@ Collector PID            : 37266
@ Final Output Dir         : .
@ MPI Task Assignment      : 0 snow06.llnl.gov
@ MPI Task Assignment      : 1 snow06.llnl.gov
@ MPI Task Assignment      : 2 snow06.llnl.gov
@ MPI Task Assignment      : 3 snow06.llnl.gov

   This  next  section  provides an overview of the application's time in
   MPI. Apptime is the wall-clock time from the end of MPI_Init until the
   beginning of MPI_Finalize. MPI_Time is the wall-clock time for all the
   MPI  calls  contained  within  Apptime.  MPI%  shows the ratio of this
   MPI_Time  to  Apptime.  The asterisk (*) is the aggregate line for the
   entire application.
---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime    MPI%
   0         10   0.000243    0.00
   1         10         10   99.92
   2         10         10   99.92
   3         10         10   99.92
   *         40         30   74.94

   The  callsite  section  identifies  all  the  MPI callsites within the
   application.  The  first number is the callsite ID for this mpiP file.
   The  next column shows the type of MPI call (w/o the MPI_ prefix). The
   name  of the function that contains this MPI call is next, followed by
   the  file name and line number. Finally, the last column shows the PC,
   or  program  counter,  for  that  MPI  callsite. Note that the default
   setting  for  callsite  stack  walk  depth  is  1. Other settings will
   enumerate  callsites  by the entire stack trace rather than the single
   callsite alone.
---------------------------------------------------------------------------
@--- Callsites: 2 ---------------------------------------------------------
---------------------------------------------------------------------------
 ID Lev File            Line Parent_Funct         MPI_Call
  1   0 9-test-mpip-time.c   47 .main                          Barrier
  2   0 9-test-mpip-time.c   56 .main                          Barrier

   The  aggregate time section is a very quick overview of the top twenty
   MPI   callsites   that   consume  the  most  aggregate  time  in  your
   application.  Call  identifies the type of MPI function. Site provides
   the  callsite  ID  (as  listed  in  the callsite section). Time is the
   aggregate  time  for  that  callsite  in  milliseconds.  The final two
   columns show the ratio of that aggregate time to the total application
   time and to the total MPI time, respectively.
---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site      Time     App%     MPI%
Barrier                 2      3e+04   74.94   100.00
Barrier                 1      0.547    0.00     0.00

   The next section is similar to the aggregate time section, although it
   reports  on  the  top  20  callsites for total sent message sizes. For
   example:
---------------------------------------------------------------------------
@--- Aggregate Sent Message Size (top twenty, descending, bytes) ----------
---------------------------------------------------------------------------
Call                 Site      Count      Total    Avrg      MPI%
Send                    7        320   1.92e+06   6e+03     99.96
Bcast                   1         12        336      28      0.02

   The  final  sections  are the ad nauseum listing of the statistics for
   each  callsite  across  all  tasks,  followed  by  an  aggregate  line
   (indicated  by  an  asterisk in the Rank column). The first section is
   for operation time followed by the section for message sizes.
---------------------------------------------------------------------------
@--- Callsite Time statistics (all, milliseconds): 8 ----------------------
---------------------------------------------------------------------------
Name              Site Rank  Count      Max     Mean      Min   App%   MPI%
Barrier              1    0      1    0.107    0.107    0.107   0.00  44.03
Barrier              1    *      4    0.174    0.137    0.107   0.00   0.00

Barrier              2    0      1    0.136    0.136    0.136   0.00  55.97
Barrier              2    1      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    2      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    3      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    *      4    1e+04  7.5e+03    0.136  74.94 100.00

   Remember  that  we  configured  MPIP to not print lines where MPI% was
   less  than  10%.  All  aggregate  lines  are printed regardless of the
   configuration settings.

   Column                           Description
   Name   Name of the MPI function at that callsite.
   Site   Callsite ID as listed in the callsite section above.
   Rank   Task rank in MPI_COMM_WORLD.
   Count  Number of times this call was executed.
   Max    Maximum wall-clock time for one call.
   Mean   Arithmetic mean of the wall-clock time for one call.
   Min    Minimum wall-clock time for one call.
   App%   Ratio of time for this call to the overall application time for
          each task.
   MPI%   Ratio of time for this call to the overall MPI time for each
          task.

   The  aggregate  result for each call has the same measurement meaning;
   however,  the  statistics  are  gathered across all tasks and compared
   with the aggregate application and MPI times.

   The section for sent message sizes has a similar format:
---------------------------------------------------------------------------
@--- Callsite Message Sent statistics (all, sent bytes) -------------------
---------------------------------------------------------------------------
Name              Site Rank   Count       Max      Mean       Min       Sum
Send                 5    0      80      6000      6000      6000   4.8e+05
Send                 5    1      80      6000      6000      6000   4.8e+05
Send                 5    2      80      6000      6000      6000   4.8e+05
Send                 5    3      80      6000      6000      6000   4.8e+05
Send                 5    *     320      6000      6000      6000   1.92e+06

   Column                           Description
   Name   Name of the MPI function at that callsite.
   Site   Callsite ID as listed in the callsite section above.
   Rank   Task rank in MPI_COMM_WORLD.
   Count  Number of times this call was executed.
   Max    Maximum sent message size in bytes for one call.
   Mean   Arithmetic mean of the sent message sizes in bytes for one call.
   Min    Minimum sent message size in bytes for one call.
   Sum    Total of all message sizes for this operation and callsite.

   The  format  of  MPI  I/O  report  section is very similar to the sent
   message sizes section:
---------------------------------------------------------------------------
@--- Callsite I/O statistics (all, I/O bytes) -----------------------------
---------------------------------------------------------------------------
Name              Site Rank   Count       Max      Mean       Min       Sum
File_read            1    0      20        64        64        64      1280
File_read            1    1      20        64        64        64      1280
File_read            1    *      40        64        64        64      2560

  Report Viewers

   Several  projects  are developing tools for viewing and analyzing mpiP
   report data:
     * The [31]Tool Gear project has a Qt mpiP viewer.
     * The  [32]TAU project will support mpiP report viewing and database
       upload  of  mpiP data from within the ParaProf tool as of a 8/6/04
       release with documentation updates to follow.

   [33]Top
            ____________________________________________________

Controlling the Scope of mpiP Profiling in your Application

   In mpiP, you can limit the scope of profiling measurements to specific
   regions  of  your code using the MPI_Pcontrol(int level) subroutine. A
   value of zero disables mpiP profiling, while any nonzero value enables
   profiling.  To  disable  profiling  initially  at MPI_Init, use the -o
   configuration  option.  mpiP  will  only  record information about MPI
   commands  encountered between activation and deactivation. There is no
   limit  to  the  number  to  times  that  an  application  can activate
   profiling during execution.

   For  example, in your application you can capture the MPI activity for
   timestep  5  only using Pcontrol. Remember to set the mpiP environment
   variable to include -o when using this feature.
for(i=1; i < 10; i++)
{
  switch(i)
  {
    case 5:
      MPI_Pcontrol(1);
      break;
    case 6:
      MPI_Pcontrol(0);
      break;
    default:
      break;
  }
  /* ... compute and communicate for one timestep ... */
}

   You   can   also   generate  arbitrary  reports  by  making  calls  to
   MPI_Pcontrol(2).  The  first  report  generated  will have the default
   report  filename.  Subsequent  report  files will have an index number
   included, such as sweep3d.mpi.4.7371.1.mpiP,
   sweep3d.mpi.4.7371.2.mpiP,etc.   The   final   report  will  still  be
   generated during MPI_Finalize.

   MPI_Pcontrol  features should be fully functional for C/C++ as well as
   Fortran.

   [34]Top
            ____________________________________________________

Caveats

     * If  mpiP  has problems with the source code translation, you might
       be  able  to decode the program counters on LLNL systems with some
       of  the  following  techniques. You can use instmap, addr2line, or
       look at the assembler code itself.
     * Compiler  transformations  like  loop unrolling can sometimes make
       one  source code line appear as many different PCs. You can verify
       this  by  looking at the assembler. In my experience, both instmap
       and  addr2line  do  a pretty good job of mapping these transformed
       PCs into a file name and line number.
          + instmap--an IBM utility
          + addr2line--a gnu tool
          + look at the assembler listing, or with GNU's objdump (-d -S)
          + use Totalview or gdb to translate the PC
     * There  are  known incompatibilities with certain binutils versions
       and  recent  versions  of the IBM compilers. As of this release, a
       fix  has  not  been incorporated into binutils, however, using the
       -bnoobjreorder option is a valid work-around.
     * In  one  case, we encountered problems on IBM machines with source
       lookup   of  64-bit  Fortran  applications.  It  appears  that  an
       incorrect  compiler configuration file was being used, incorrectly
       matching debugging information and PC values. We addressed this by
       using the link flag -bpT:0x100000000.
     * Issues when stack walking optimized applications:
          + Applications  compiled  with  gcc may return incorrect parent
            functions;  however, the file and line number information may
            be correct.
          + Applications compiled with the Intel compiler may not be able
            to identify parent stack frames.
     * If  you  are  calling MPI functions from within dynamically loaded
       objects, you may need to recompile the library as a shared object.
     * The  mpiP  library  will  not  link  with the signal-based IBM mpi
       library.
     * We  have  encountered  occaisional negative report values on Linux
       and  AIX  systems. We will continue to investigate this issue, but
       it  is  possible  that  this behavior may be experienced with mpiP
       v2.7.

   [35]Top
             __________________________________________________

MPI Routines Profiled with mpiP

   MPI_Allgather
   MPI_Allgatherv
   MPI_Allreduce
   MPI_Alltoall
   MPI_Alltoallv
   MPI_Attr_delete
   MPI_Attr_get
   MPI_Attr_put
   MPI_Barrier
   MPI_Bcast
   MPI_Bsend
   MPI_Bsend_init
   MPI_Buffer_attach
   MPI_Buffer_detach
   MPI_Cancel
   MPI_Cart_coords
   MPI_Cart_create
   MPI_Cart_get
   MPI_Cart_map
   MPI_Cart_rank
   MPI_Cart_shift
   MPI_Cart_sub
   MPI_Cartdim_get
   MPI_Comm_create
   MPI_Comm_dup
   MPI_Comm_group
   MPI_Comm_remote_group
   MPI_Comm_remote_size
   MPI_Comm_split
   MPI_Comm_test_inter
   MPI_Dims_create
   MPI_Error_class
   MPI_File_close
   MPI_File_open
   MPI_File_preallocate
   MPI_File_read
   MPI_File_read_all
   MPI_File_read_at
   MPI_File_seek
   MPI_File_set_view
   MPI_File_write
   MPI_File_write_all
   MPI_File_write_at
   MPI_Gather
   MPI_Gatherv
   MPI_Graph_create
   MPI_Graph_get
   MPI_Graph_map
   MPI_Graph_neighbors
   MPI_Graph_neighbors_count
   MPI_Graphdims_get
   MPI_Group_compare
   MPI_Group_difference
   MPI_Group_excl
   MPI_Group_free
   MPI_Group_incl
   MPI_Group_intersection
   MPI_Group_translate_ranks
   MPI_Group_union
   MPI_Ibsend
   MPI_Intercomm_create
   MPI_Intercomm_merge
   MPI_Iprobe
   MPI_Irecv
   MPI_Irsend
   MPI_Isend
   MPI_Issend
   MPI_Keyval_create
   MPI_Keyval_free
   MPI_Pack
   MPI_Probe
   MPI_Recv
   MPI_Recv_init
   MPI_Reduce
   MPI_Reduce_scatter
   MPI_Request_free
   MPI_Rsend
   MPI_Rsend_init
   MPI_Scan
   MPI_Scatter
   MPI_Scatterv
   MPI_Send
   MPI_Send_init
   MPI_Sendrecv
   MPI_Sendrecv_replace
   MPI_Ssend
   MPI_Ssend_init
   MPI_Start
   MPI_Startall
   MPI_Test
   MPI_Testall
   MPI_Testany
   MPI_Testsome
   MPI_Topo_test
   MPI_Type_commit
   MPI_Type_free
   MPI_Type_get_contents
   MPI_Type_get_envelope
   MPI_Unpack
   MPI_Wait
   MPI_Waitall
   MPI_Waitany
   MPI_Waitsome

   [36]Top
            ____________________________________________________

MPI Routines For Which mpiP Gathers Sent Message Size Data

   MPI_Allgather
   MPI_Allgatherv
   MPI_Allreduce
   MPI_Alltoall
   MPI_Bcast
   MPI_Bsend
   MPI_Gather
   MPI_Gatherv
   MPI_Ibsend
   MPI_Irsend
   MPI_Isend
   MPI_Issend
   MPI_Reduce
   MPI_Rsend
   MPI_Scan
   MPI_Scatter
   MPI_Send
   MPI_Sendrecv
   MPI_Sendrecv_replace
   MPI_Ssend

   [37]Top
            ____________________________________________________

MPI Routines For Which mpiP Gathers I/O Data

   MPI_File_close
   MPI_File_open
   MPI_File_preallocate
   MPI_File_read
   MPI_File_read_all
   MPI_File_read_at
   MPI_File_seek
   MPI_File_set_view
   MPI_File_write
   MPI_File_write_all
   MPI_File_write_at

   [38]Top
     _________________________________________________________________

   For further information please send mail to [39]mpip-help@llnl.gov.

   Last modified on August 3rd, 2004.
   UCRL-CODE-2002-020, Version 2.

References

   1. http://www.llnl.gov/disclaimer.html
   2. mailto:vetterjs@ornl.gov
   3. mailto:chcham@llnl.gov
   4. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Introduction
   5. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Downloading
   6. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Contributing
   7. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#New_Features
   8. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Using
   9. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Supported
  10. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Building
  11. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Linking_Examples
  12. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Runtime_Configuration
  13. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#mpiP_Output
  14. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Report_Viewers
  15. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Controlling_Scope
  16. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Caveats
  17. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Profiled_Routines
  18. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#Message_Size_Routines
  19. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#IO_Routines
  20. mailto:mpip-help@llnl.gov
  21. mailto:majordomo@llnl.gov
  22. mailto:mpip-usersp@llnl.gov
  23. http://www.llnl.gov/CASC/people/vetter/vetter_pubs.html
  24. http://www.llnl.gov/CASC/mpip/download/
  25. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  26. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  27. http://sources.redhat.com/binutils
  28. ftp://sources.redhat.com/pub/binutils/snapshots
  29. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  30. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  31. http://www.llnl.gov/CASC/tool_gear/
  32. http://www.cs.uoregon.edu/research/paracomp/tau/tautools/
  33. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  34. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  35. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  36. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  37. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  38. file://localhost/mnt/win_d/LabWork/ToolWeb/mpiP-2.7.html#top
  39. mailto:mpip-help@llnl.gov
