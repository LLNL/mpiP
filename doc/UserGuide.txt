
                 mpiP: Lightweight, Scalable MPI Profiling

                                Version 2.8
                              18 January 2005
                               Jeffrey Vetter
                             vetterjs@ornl.gov
                              Chris Chambreau
                              chcham@llnl.gov
   ______________________________________________________________________

Contents

     * Introduction
          + Downloading
          + Contributing
          + New Features
     * Using mpiP (summary)
          + Supported Platforms
     * Building mpiP
          + Linking Examples
     * Run-time Configuration
     * mpiP Output

     * Report Viewers

     Controlling mpiP Profiling Scope

     Caveats

     List of Profiled Routines

     List of Routines With Collected Sent Message Size Information

     List of I/O Routines

            ____________________________________________________

Introduction

   mpiP  is a lightweight profiling library for MPI applications. Because
   it  only  collects  statistical  information about MPI functions, mpiP
   generates  considerably  less overhead and much less data than tracing
   tools.  All  the  information  captured by mpiP is task-local. It only
   uses  communication  during report generation, typically at the end of
   the experiment, to merge results from all of the tasks into one output
   file.

   We  have tested mpiP on a variety of C/C++/Fortran applications from 2
   to  4096  processes,  including  a  3584-process  run  on ASCI Q and a
   4096-process  run on ASCI White. Please send your comments, questions,
   and  ideas  for  enhancements  to  mpip-help@llnl.gov. To receive mail
   regarding     new     mpiP     releases,     please    subscribe    to
   mpip-announce@llnl.gov    (send    e-mail    with    body   "subscribe
   mpip-announce"    to   majordomo@llnl.gov).   Please   also   consider
   subscribing  to mpip-users@llnl.gov to contribute and receive mpiP use
   and status information.

   To  learn  more about performance analysis with mpiP, see Vetter, J.S.
   and M.O. McCracken, "Statistical Scalability Analysis of Communication
   Operations  in  Distributed  Applications," Proc. ACM SIGPLAN Symp. on
   Principles and Practice of Parallel Programming (PPOPP), 2001.

  Downloading

   You    may    download    the    current    version   of   mpiP   from
   http://www.llnl.gov/CASC/mpip/download/

  Contributing

   We are constantly improving mpiP. Bug fixes and ports to new platforms
   are always welcome. Many thanks to the following contributors:
     * Michael McCracken (UCSD)
     * Curt Janssen (Sandia National Laboratories)
     * Mike Campbell (UIUC)
     * Jim Brandt (Sandia National Laboratories)
     * Philip Roth (Oak Ridge National Laboratory)

  New Features with v2.8

   Release  v2.8  corrects  some  outstanding  issues  and  provides  the
   following new features:
     * An  API  (see  mpiP-API.c and mpiP-API.h) is available through the
       mpiP library for the following functionality:
          + stackwalking
          + address-to-source translation
          + symbol demangling
          + timing routines
          + accessing the name of the executable
     * New  platform and compiler support, including IBM ppc64-BG/L, Cray
       X1 & XD1, SGI Altix, and PGI compilers.
     * Scripts   are  provided  to  use  addr2line  to  translate  report
       addresses  to source information and demonstrate runtime profiling
       insertion.
     * A make install target has been implemented.
     * The   standard  report  filename  format  is  now  [executable  or
       "Unknown"].[process  count].[rank 0 PID].[report index].mpiP. When
       generating reports, mpiP will not overwrite existing report files.
       The  report  index  will be incremented until a unique filename is
       found.  The  report  index is also incremented if multiple reports
       are generated within a single run.
     * The  default  Linux  timer is gettimeofday. Either gettimeofday or
       clock_gettime  can  be specified as the timer routine used by mpiP
       with a configure option.
     * To avoid attempting to run tests during configure, cross_compiling
       is set.
     * COV  calculation  is  now the default. It can be disabled with the
       "-c" environment variable option.
     * Previous  Linux  Fortran/C/C++  libraries  have been merged into a
       single library.
     * Greater configuration flexibility. See configure --help.

            ____________________________________________________

Using mpiP

   Using  mpiP is very simple. Because it gathers MPI information through
   the  MPI  profiling  layer,  mpiP is a link-time library. That is, you
   don't  have  to  recompile your application to use mpiP. Note that you
   might  have to recompile to include the '-g' option. This is important
   if  you  want mpiP to decode the PC to a source code filename and line
   number automatically. mpiP will work without -g, but mileage may vary.

   To compile a simple program on LLNL AIX, you need to add the following
   libraries to your compile/link line:

     -L${mpiP_root}/lib -lmpiP -lbfd -liberty -lintl -lm

   For example, the new mpiP link line becomes

     $  mpcc  -g  -O  mpi-foo.c -o mpi-foo.exe -L${mpiP_root}/lib -lmpiP
     -lbfd -liberty -lintl -lm

   from

     $ mpcc -O mpi-foo.c -o mpi-foo.exe

   Make sure the mpiP library appears before the MPI library on your link
   line.  The three libraries (-lbfd -liberty -lintl) provide support for
   decoding the symbol information; they are part of GNU binutils.

   Run  your  application.  You  can  verify  that  mpiP  is  working  by
   identifying the header and trailer in standard out.

     0:mpiP:
     0:mpiP: mpiP V2.8.0 (Build Jan 7 2005/10:59:14)
     0:mpiP: Direct questions and errors to mpip-help@llnl.gov>
     0:mpiP:
     0:mpiP:
     0:mpiP: found 21557 symbols in file [./9-test-mpip-time.exe]
     0:mpiP:
     0:mpiP: Storing mpiP output in
     [./9-test-mpip-time.exe.4.37266.1.mpiP].
     0:mpiP:

   By default, the output file is written to the current directory of the
   application.  mpiP  files are always much smaller than trace files, so
   writing them to this directory is safe.

  Supported Platforms

   The  2.8  release  of mpiP supports Linux, Tru64, and AIX and has been
   tested  on  UNICOS and IBM BG/L. Please contact us with bug reports or
   questions regarding these platforms. Note that mpiP will not work with
   MPICH  1.1  nor  with  the  IBM signal-based MPI library (libmpi.a, as
   opposed  to the thread-based implementation libmpi_r.a). The following
   table  indicates  platforms  where  mpiP  was  succesfully run and any
   requirements for that platform.

   Platform OS Compiler MPI binutils Requirements
   IA32-Linux 2.4.21 CHAOS Kernel Intel 8.1
   PGI  5.1-6  Quadrics  MPI 1.24-38 2.15 Some binutils mods required for
   Intel  8.0  support.  Please  send  mail  to  the  help  list for more
   information.
   2.4.22 Kernel gcc 3.3.1 mpich2-1.0
   IBM Power3/4 AIX 5.1
   AIX 5.2 VA C 6.0/
   VA Fortran 8.1 PE 3.2
   PE  4.1  2.15 Binutils currently requires that applications are linked
   with  the  -bnoobjreorder  linker  flag.  Other  mods are required for
   64-bit builds of binutils.
   Alpha EV67 Tru64 5.1b Compaq C 6.4/
   Fortran 5.5 Quadrics RMS 2.5 2.14 2.15

            ____________________________________________________

Building mpiP

   Currently,  mpiP  requires a compatible GNU binutils installation. The
   binutils  include  and  lib  directories  must  be  specified with the
   --with-include  and  --with-ldflags configure flags. It is very likely
   that  the  compilers  will  need  to  be indentified as well, with the
   --with-cc,  --with-cxx,  and --with-f77 flags. After running configure
   with  the  appropriate  arguments,  "make"  will build the appropriate
   libraries in the mpiP directory.

   There  are  many  new  configuration  options  available.  They are as
   follows:

   Flag Effect Description
   --enable-demangling Specify demangling support. If the GNU option is
   specified, demangling is applied to each symbol by default using the
   libiberty implementation. For the IBM and Compaq options, demangling
   support is implemented in the library libmpiPdmg.a. Use GNU for the
   Intel compiler.
   --enable-stackdepth  Specify  maximum stacktrace depth (default is 8).
   Stacktraces  with  larger  than  8 levels are sometime useful for some
   applications.
   --enable-dwarf  Use  libdwarf/libelf  for  source lookup. libdwarf and
   libelf can be used for address-to-source translation as an alternative
   to binutils libbfd.
   --disable-bfd  Do  not  use  GNU  binutils  libbfd  for source lookup.
   Binutils is not always available or compatible.
   --disable-libunwind  Do  not  use  libunwind to generate stack traces.
   Currently,   libunwind   seems  useful  on  IA64-Linux  and  x86-Linux
   platforms, although it can conflict with the libunwind.a provided with
   the Intel compiler.
   --disable-mpi-io  Disable  MPI-I/O reporting. Useful for generating an
   mpiP  library without MPI I/O for MPI implementations such as Quadrics
   MPI that has a separate MPI I/O library.
   --with-gettimeofday  Use gettimeofday for timing. Use the gettimeofday
   call for timing instead of the default platform timer.
   --with-clock_gettime   Use   clock_gettime   for   timing.   Use   the
   clock_gettime  high-resolution per-process timer for timing instead of
   the default platform timer.
   --enable-check-time   Enable  AIX  check  for  negative  time  values.
   Activate IBM timing debugging code.
   --enable-getarg  Use getarg to get fortran command line args. This was
   used  with  the  Intel  8.0  compiler in order to identify the name of
   Fortran  applications.  This  should  no  longer  be  needed given new
   application  name  identification support. This option will be removed
   in the next release.

  Example Application Link Commands for LLNL Machines

   LLNL  Linux  users  (MCR/Pengra/ALC/Thunder) can now use the srun-mpip
   wrapper  script  to  use mpiP without re-linking their application. An
   example script for mpirun is provided in the mpiP bin directory.

   OS Compiler Language Example Link Command
   AIX Visual Age C mpxlc -g -bnoobjreorder 1-hot-potato.c -o
   1-hot-potato.exe -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty
   -lintl -lm
   C++   mpCC_r   -g   -bnoobjreorder   4-demangle.C   -o  4-demangle.exe
   -L/usr/local/tools/mpiP/lib -lmpiPdmg -lbfd -liberty -lintl -lm
   Fortran   mpxlf   -g   -bnoobjreorder   sweep-ops.f  -o  sweep-ops.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lintl -lm
   Linux   Intel   C   mpiicc   -g   1-hot-potato.c  -o  1-hot-potato.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty-lm -lmpio
   C++ mpiicc -g 4-demangle.C -o 4-demangle.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty-lm -lmpio
   Fortran      mpiifc      -g      sweep-ops.f      -o     sweep-ops.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lm -lmpio
   PGI     C    mpipgcc    -g    1-hot-potato.c    -o    1-hot-potato.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lm -lmpio
   C++      mpipgCC      -g      4-demangle.C      -o      4-demangle.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lm -lmpio
   Fortran      mpipgf77      -g     sweep-ops.f     -o     sweep-ops.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lm -lmpio
   GNU     C     mpicc     -g    1-hot-potato.c    -o    1-hot-potato.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lm
   C++ mpiCC -g 4-demangle.C -o 4-demangle.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lm
   Fortran      mpif77      -g      sweep-ops.f      -o     sweep-ops.exe
   -L/usr/local/tools/mpiP/lib -lmpiP -lbfd -liberty -lm
   Tru64   Compaq   C  cc  -g  -I/usr/lib/mpi/include  1-hot-potato.c  -o
   1-hot-potato.exe  -L/usr/local/tools/mpiP/lib  -lmpiP  -lbfd  -liberty
   -lintl -lm -lmpi -lpmpi -lexc
   C++  cxx -g 4-demangle.C -o 4-demangle.exe -L/usr/local/tools/mpiP/lib
   -lmpiPdmg -lbfd -liberty -lintl -lm -lmpi -lpmpi -lexc -lmld
   Fortran  f77  -g  -I/usr/lib/mpi/include  sweep-ops.f -o sweep-ops.exe
   -L/usr/local/tools/mpiP/lib  -lmpiP  -lbfd  -liberty  -lintl -lm -lmpi
   -lpmpi -lexc

   Note:
     * On  the LLNL ia64-Linux machine Thunder, /usr/lib/libunwind.a must
       be  added  to  the link line so that the non-Intel library is used
       for  stack  unwinding. It may also be necessary to add "-ldl" when
       linking on this system.
     * On  Tru64, the exception library flag (-lexc) must be added to the
       link command.
     * Source  lookup  for  callsites  may  fail with certain versions of
       binutils.  If  you  are  running  into  trouble,  you  may want to
       download a recent snapshot from
       ftp://sources.redhat.com/pub/binutils/snapshots.

            ____________________________________________________

Run-time Configuration of mpiP

   mpiP  has  several configurable parameters that a user can set via the
   environment   variable   MPIP.   The   setting  for  MPIP  looks  like
   command-line  parameters:  "-t  10  -k 2". Currently, mpiP has several
   configurable parameters.

   Option Description Default
   -c  Disable COV calculation field for indicating process distribution.
   enabled
   -e Print report data using floating-point format.
   -f dir Record output file in directory <dir>. .
   -g Enable mpiP debug mode. disabled
   -k n Sets callsite stack traceback depth to <n>. 1
   -n Do not truncate full pathname of filename in callsites.
   -o  Disable  profiling  at  initialization.  Application  must  enable
   profiling with MPI_Pcontrol().
   -s n Set hash table size to <n>. 256
   -t  x  Set print threshold for report, where <x> is the MPI percentage
   of time for each callsite. 0.0

   For  example,  to  set  the  callsite stack walking depth to 2 and the
   report  print  threshold  to  10%,  you simply need to define the mpiP
   string in your environment:

   $ export MPIP="-t 10.0 -k 2"

   mpiP  prints a message at initialization if it successfully finds this
   MPIP variable.

            ____________________________________________________

mpiP Output

   Here  is  some  sample output from mpiP with an application that has 4
   MPI  calls.  It  is  broken  down  by sections below. Here also is the
   experiment  setup.  Note  that MPIP does not capture information about
   ALL  MPI  calls.  Local calls, such as MPI_Comm_size, are omitted from
   the  profiling  library  measurement  to  reduce perturbation and mpiP
   output.

   The test code:
  sleeptime = 10;
  MPI_Init (&argc, &argv);
  MPI_Comm_size (comm, &nprocs);
  MPI_Comm_rank (comm, &rank);
  MPI_Barrier (comm);
  if (rank == 0)
    {
      sleep (sleeptime);        /* slacker! delaying everyone else */
    }
  MPI_Barrier (comm);
  MPI_Finalize ();

   The code was compiled with:
  $ mpcc -g -g -DAIX 9-test-mpip-time.c -o 9-test-mpip-time.exe  \
       -L.. -L/g/g2/vetter/AIX/lib  -lmpiP -lbfd -liberty -lintl -lm

   Environment variables were set as:
  $ export MPIP="-t 10.0"

   The example was executed on MCR like this:
  $ srun -n 4 -ppdebug ./9-test-mpip-time.exe

   This experiment produced an output file that we can now analyze:
 ./9-test-mpip-time.exe.4.25972.1.mpiP

   Header  information  provides basic information about your performance
   experiment.

@ mpiP
@ Command : /g/g0/chcham/mpiP/devo/testing/./9-test-mpip-time.exe
@ Version                  : 2.8.0
@ MPIP Build date          : Jan 10 2005, 15:15:47
@ Start time               : 2005 01 10 16:01:32
@ Stop time                : 2005 01 10 16:01:42
@ Timer Used               : gettimeofday
@ MPIP env var             : -t 10.0
@ Collector Rank           : 0
@ Collector PID            : 25972
@ Final Output Dir         : .
@ MPI Task Assignment      : 0 mcr88
@ MPI Task Assignment      : 1 mcr88
@ MPI Task Assignment      : 2 mcr89
@ MPI Task Assignment      : 3 mcr89

   This  next  section  provides an overview of the application's time in
   MPI. Apptime is the wall-clock time from the end of MPI_Init until the
   beginning of MPI_Finalize. MPI_Time is the wall-clock time for all the
   MPI  calls  contained  within  Apptime.  MPI%  shows the ratio of this
   MPI_Time  to  Apptime.  The asterisk (*) is the aggregate line for the
   entire application.

---------------------------------------------------------------------------
@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime    MPI%
   0         10   0.000243    0.00
   1         10         10   99.92
   2         10         10   99.92
   3         10         10   99.92
   *         40         30   74.94

   The  callsite  section  identifies  all  the  MPI callsites within the
   application.  The  first number is the callsite ID for this mpiP file.
   The  next column shows the type of MPI call (w/o the MPI_ prefix). The
   name  of the function that contains this MPI call is next, followed by
   the  file name and line number. Finally, the last column shows the PC,
   or  program  counter,  for  that  MPI  callsite. Note that the default
   setting  for  callsite  stack  walk  depth  is  1. Other settings will
   enumerate  callsites  by the entire stack trace rather than the single
   callsite alone.

---------------------------------------------------------------------------
@--- Callsites: 2 ---------------------------------------------------------
---------------------------------------------------------------------------
 ID Lev File/Address        Line Parent_Funct             MPI_Call
  1   0 9-test-mpip-time.c    52 main                     Barrier
  2   0 9-test-mpip-time.c    61 main                     Barrier

   The  aggregate time section is a very quick overview of the top twenty
   MPI   callsites   that   consume  the  most  aggregate  time  in  your
   application.  Call  identifies the type of MPI function. Site provides
   the  callsite  ID  (as  listed  in  the callsite section). Time is the
   aggregate time for that callsite in milliseconds. The next two columns
   show  the  ratio  of that aggregate time to the total application time
   and  to the total MPI time, respectively. The COV column indicates the
   variation  in  times  of  individual  processes  for  this callsite by
   presenting  the  coefficient  of  variation  as  calculated  from  the
   individual  process  times.  A  larger  value indicates more variation
   between the process times.

---------------------------------------------------------------------------
@--- Aggregate Time (top twenty, descending, milliseconds) ----------------
---------------------------------------------------------------------------
Call                 Site       Time    App%    MPI%     COV
Barrier                 2      3e+04   75.00  100.00    0.67
Barrier                 1      0.405    0.00    0.00    0.59

   The next section is similar to the aggregate time section, although it
   reports  on  the  top  20  callsites for total sent message sizes. For
   example:

---------------------------------------------------------------------------
@--- Aggregate Sent Message Size (top twenty, descending, bytes) ----------
---------------------------------------------------------------------------
Call                 Site      Count      Total    Avrg      MPI%
Send                    7        320   1.92e+06   6e+03     99.96
Bcast                   1         12        336      28      0.02

   The  final  sections  are the ad nauseum listing of the statistics for
   each  callsite  across  all  tasks,  followed  by  an  aggregate  line
   (indicated  by  an  asterisk in the Rank column). The first section is
   for operation time followed by the section for message sizes.

---------------------------------------------------------------------------
@--- Callsite Time statistics (all, milliseconds): 8 ----------------------
---------------------------------------------------------------------------
Name              Site Rank  Count      Max     Mean      Min   App%   MPI%
Barrier              1    0      1    0.107    0.107    0.107   0.00  44.03
Barrier              1    *      4    0.174    0.137    0.107   0.00   0.00

Barrier              2    0      1    0.136    0.136    0.136   0.00  55.97
Barrier              2    1      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    2      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    3      1    1e+04    1e+04    1e+04  99.92 100.00
Barrier              2    *      4    1e+04  7.5e+03    0.136  74.94 100.00

   Remember  that  we  configured  MPIP to not print lines where MPI% was
   less  than  10%.  All  aggregate  lines  are printed regardless of the
   configuration settings.

   Column                           Description
   Name   Name of the MPI function at that callsite.
   Site   Callsite ID as listed in the callsite section above.
   Rank   Task rank in MPI_COMM_WORLD.
   Count  Number of times this call was executed.
   Max    Maximum wall-clock time for one call.
   Mean   Arithmetic mean of the wall-clock time for one call.
   Min    Minimum wall-clock time for one call.
   App%   Ratio of time for this call to the overall application time for
          each task.
   MPI%   Ratio of time for this call to the overall MPI time for each
          task.

   The  aggregate  result for each call has the same measurement meaning;
   however,  the  statistics  are  gathered across all tasks and compared
   with the aggregate application and MPI times.

   The section for sent message sizes has a similar format:

---------------------------------------------------------------------------
@--- Callsite Message Sent statistics (all, sent bytes) -------------------
---------------------------------------------------------------------------
Name              Site Rank   Count       Max      Mean       Min       Sum
Send                 5    0      80      6000      6000      6000   4.8e+05
Send                 5    1      80      6000      6000      6000   4.8e+05
Send                 5    2      80      6000      6000      6000   4.8e+05
Send                 5    3      80      6000      6000      6000   4.8e+05
Send                 5    *     320      6000      6000      6000   1.92e+06

   Column                           Description
   Name   Name of the MPI function at that callsite.
   Site   Callsite ID as listed in the callsite section above.
   Rank   Task rank in MPI_COMM_WORLD.
   Count  Number of times this call was executed.
   Max    Maximum sent message size in bytes for one call.
   Mean   Arithmetic mean of the sent message sizes in bytes for one call.
   Min    Minimum sent message size in bytes for one call.
   Sum    Total of all message sizes for this operation and callsite.

   The  format  of  MPI  I/O  report  section is very similar to the sent
   message sizes section:

---------------------------------------------------------------------------
@--- Callsite I/O statistics (all, I/O bytes) -----------------------------
---------------------------------------------------------------------------
Name              Site Rank   Count       Max      Mean       Min       Sum
File_read            1    0      20        64        64        64      1280
File_read            1    1      20        64        64        64      1280
File_read            1    *      40        64        64        64      2560

  Report Viewers

     * The  Tool  Gear  project  has a Qt mpiP viewer. LLNL users can run
       this as mpipview.

            ____________________________________________________

Controlling the Scope of mpiP Profiling in your Application

   In mpiP, you can limit the scope of profiling measurements to specific
   regions  of  your code using the MPI_Pcontrol(int level) subroutine. A
   value of zero disables mpiP profiling, while any nonzero value enables
   profiling.  To  disable  profiling  initially  at MPI_Init, use the -o
   configuration  option.  mpiP  will  only  record information about MPI
   commands  encountered between activation and deactivation. There is no
   limit  to  the  number  to  times  that  an  application  can activate
   profiling during execution.

   For  example, in your application you can capture the MPI activity for
   timestep  5  only using Pcontrol. Remember to set the mpiP environment
   variable to include -o when using this feature.

for(i=1; i < 10; i++)
{
  switch(i)
  {
    case 5:
      MPI_Pcontrol(1);
      break;
    case 6:
      MPI_Pcontrol(0);
      break;
    default:
      break;
  }
  /* ... compute and communicate for one timestep ... */
}

  Arbitrary Report Generation

   You   can   also   generate  arbitrary  reports  by  making  calls  to
   MPI_Pcontrol(2).  The  first  report  generated  will have the default
   report  filename.  Subsequent  report  files will have an index number
   included, such as sweep3d.mpi.4.7371.1.mpiP,
   sweep3d.mpi.4.7371.2.mpiP,etc.   The   final   report  will  still  be
   generated  during MPI_Finalize. NOTE: In the current release, callsite
   IDs  will  not  be  consistent between reports. Comparison of callsite
   data between reports must be done by source location and callstack.

   MPI_Pcontrol  features should be fully functional for C/C++ as well as
   Fortran.

            ____________________________________________________

Caveats

     * If  mpiP  has problems with the source code translation, you might
       be  able  to decode the program counters on LLNL systems with some
       of  the  following  techniques. You can use instmap, addr2line, or
       look at the assembler code itself.
     * Compiler  transformations  like  loop unrolling can sometimes make
       one  source code line appear as many different PCs. You can verify
       this  by  looking at the assembler. In my experience, both instmap
       and  addr2line  do  a pretty good job of mapping these transformed
       PCs into a file name and line number.
          + instmap--an IBM utility
          + addr2line--a gnu tool
          + look at the assembler listing, or with GNU's objdump (-d -S)
          + use Totalview or gdb to translate the PC
     * There  are  known incompatibilities with certain binutils versions
       and  recent  versions  of the IBM compilers. As of this release, a
       fix  has  not  been incorporated into binutils, however, using the
       -bnoobjreorder option is a valid work-around.
     * In  one  case, we encountered problems on IBM machines with source
       lookup   of  64-bit  Fortran  applications.  It  appears  that  an
       incorrect  compiler configuration file was being used, incorrectly
       matching debugging information and PC values. We addressed this by
       using the link flag -bpT:0x100000000.
     * Issues when stack walking optimized applications:
          + Applications  compiled  with  gcc may return incorrect parent
            functions;  however, the file and line number information may
            be correct.
          + Applications compiled with the Intel compiler may not be able
            to identify parent stack frames.
     * If  you  are  calling MPI functions from within dynamically loaded
       objects, you may need to recompile the library as a shared object.
     * The  mpiP  library  will  not  link  with the signal-based IBM mpi
       library.
     * We  have  encountered  occaisional negative report values on Linux
       and  AIX  systems. We will continue to investigate this issue, but
       it  is  possible  that  this behavior may be experienced with mpiP
       v2.8.

             __________________________________________________

MPI Routines Profiled with mpiP

   MPI_Allgather
   MPI_Allgatherv
   MPI_Allreduce
   MPI_Alltoall
   MPI_Alltoallv
   MPI_Attr_delete
   MPI_Attr_get
   MPI_Attr_put
   MPI_Barrier
   MPI_Bcast
   MPI_Bsend
   MPI_Bsend_init
   MPI_Buffer_attach
   MPI_Buffer_detach
   MPI_Cancel
   MPI_Cart_coords
   MPI_Cart_create
   MPI_Cart_get
   MPI_Cart_map
   MPI_Cart_rank
   MPI_Cart_shift
   MPI_Cart_sub
   MPI_Cartdim_get
   MPI_Comm_create
   MPI_Comm_dup
   MPI_Comm_group
   MPI_Comm_remote_group
   MPI_Comm_remote_size
   MPI_Comm_split
   MPI_Comm_test_inter
   MPI_Dims_create
   MPI_Error_class
   MPI_File_close
   MPI_File_open
   MPI_File_preallocate
   MPI_File_read
   MPI_File_read_all
   MPI_File_read_at
   MPI_File_seek
   MPI_File_set_view
   MPI_File_write
   MPI_File_write_all
   MPI_File_write_at
   MPI_Gather
   MPI_Gatherv
   MPI_Graph_create
   MPI_Graph_get
   MPI_Graph_map
   MPI_Graph_neighbors
   MPI_Graph_neighbors_count
   MPI_Graphdims_get
   MPI_Group_compare
   MPI_Group_difference
   MPI_Group_excl
   MPI_Group_free
   MPI_Group_incl
   MPI_Group_intersection
   MPI_Group_translate_ranks
   MPI_Group_union
   MPI_Ibsend
   MPI_Intercomm_create
   MPI_Intercomm_merge
   MPI_Iprobe
   MPI_Irecv
   MPI_Irsend
   MPI_Isend
   MPI_Issend
   MPI_Keyval_create
   MPI_Keyval_free
   MPI_Pack
   MPI_Probe
   MPI_Recv
   MPI_Recv_init
   MPI_Reduce
   MPI_Reduce_scatter
   MPI_Request_free
   MPI_Rsend
   MPI_Rsend_init
   MPI_Scan
   MPI_Scatter
   MPI_Scatterv
   MPI_Send
   MPI_Send_init
   MPI_Sendrecv
   MPI_Sendrecv_replace
   MPI_Ssend
   MPI_Ssend_init
   MPI_Start
   MPI_Startall
   MPI_Test
   MPI_Testall
   MPI_Testany
   MPI_Testsome
   MPI_Topo_test
   MPI_Type_commit
   MPI_Type_free
   MPI_Type_get_contents
   MPI_Type_get_envelope
   MPI_Unpack
   MPI_Wait
   MPI_Waitall
   MPI_Waitany
   MPI_Waitsome

            ____________________________________________________

MPI Routines For Which mpiP Gathers Sent Message Size Data

   MPI_Allgather
   MPI_Allgatherv
   MPI_Allreduce
   MPI_Alltoall
   MPI_Bcast
   MPI_Bsend
   MPI_Gather
   MPI_Gatherv
   MPI_Ibsend
   MPI_Irsend
   MPI_Isend
   MPI_Issend
   MPI_Reduce
   MPI_Rsend
   MPI_Scan
   MPI_Scatter
   MPI_Send
   MPI_Sendrecv
   MPI_Sendrecv_replace
   MPI_Ssend

            ____________________________________________________

MPI Routines For Which mpiP Gathers I/O Data

   MPI_File_close
   MPI_File_open
   MPI_File_preallocate
   MPI_File_read
   MPI_File_read_all
   MPI_File_read_at
   MPI_File_seek
   MPI_File_set_view
   MPI_File_write
   MPI_File_write_all
   MPI_File_write_at

     _________________________________________________________________

   For further information please send mail to mpip-help@llnl.gov.

   Last modified on January 18th, 2005.
   UCRL-CODE-2002-020, Version 2.
